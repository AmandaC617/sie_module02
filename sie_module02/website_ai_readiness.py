import requests
import json
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os
from typing import Dict, List, Optional, Tuple
import streamlit as st

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: google-generativeai æœªå®‰è£ï¼ŒGemini API åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

class WebsiteAIReadinessAnalyzer:
    """ç¶²ç«™ AI å°±ç·’åº¦èˆ‡æŠ€è¡“å¥åº·åº¦åˆ†æå™¨"""
    
    def __init__(self, gemini_api_key: Optional[str] = None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SIE-Diagnostic-Tool/1.0 (contact@example.com)'
        })
        
        # åˆå§‹åŒ– Gemini API
        if gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        else:
            self.gemini_model = None
    
    def analyze_website(self, website_url: str, product_category: str = None) -> Dict:
        """åˆ†æç¶²ç«™çš„ AI å°±ç·’åº¦èˆ‡æŠ€è¡“å¥åº·åº¦"""
        if not website_url.startswith(('http://', 'https://')):
            website_url = 'https://' + website_url
        
        st.info(f"ğŸ” æ­£åœ¨åˆ†æç¶²ç«™: {website_url}")
        
        try:
            # 1. æª¢æŸ¥æ ¹æª”æ¡ˆèˆ‡ LLM éµå¾æ€§
            root_files = self._check_root_files(website_url)
            
            # 2. æª¢æŸ¥ç¶²ç«™æ¶æ§‹èˆ‡æ¬Šå¨ä¿¡è™Ÿ
            architecture_signals = self._check_architecture_signals(website_url)
            
            # 3. æª¢æŸ¥ LLM å‹å–„åº¦æŒ‡æ¨™
            llm_friendliness = self._check_llm_friendliness(website_url)
            
            # 4. æª¢æŸ¥ç”¢å“å“é¡æ¬Šå¨æ€§ (æ–°å¢)
            product_authority = self._check_product_category_authority(website_url, product_category)
            
            # 5. æª¢æŸ¥ FAQ èˆ‡æ¶ˆè²»è€…å•é¡Œè§£ç­” (æ–°å¢)
            faq_analysis = self._check_faq_and_consumer_qa(website_url, product_category)
            
            # 6. ç”Ÿæˆ AI æ”¹å–„å»ºè­°
            actionable_recommendations = self._generate_recommendations(
                root_files, architecture_signals, llm_friendliness, product_authority, faq_analysis
            )
            
            # 7. ç”Ÿæˆ SEO èˆ‡ LLM å‹å–„åº¦æ”¹å–„å»ºè­° (æ–°å¢)
            seo_llm_recommendations = self._generate_seo_llm_recommendations(
                website_url, root_files, architecture_signals, llm_friendliness, product_authority, faq_analysis
            )
            
            return {
                "technical_seo_ai_readiness": {
                    "root_files": root_files,
                    "architecture_signals": architecture_signals,
                    "llm_friendliness": llm_friendliness,
                    "product_authority": product_authority,
                    "faq_analysis": faq_analysis,
                    "actionable_recommendations": actionable_recommendations,
                    "seo_llm_recommendations": seo_llm_recommendations
                }
            }
            
        except Exception as e:
            st.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {"error": str(e)}
    
    def _check_root_files(self, website_url: str) -> Dict:
        """æª¢æŸ¥æ ¹æª”æ¡ˆèˆ‡ LLM éµå¾æ€§"""
        st.write("ğŸ“ æª¢æŸ¥æ ¹æª”æ¡ˆ...")
        
        root_files = {
            "has_robots_txt": False,
            "robots_allows_ai_bots": False,
            "has_sitemap_xml": False,
            "sitemap_is_valid": False,
            "has_llms_txt": False,
            "llms_txt_content": None
        }
        
        # æª¢æŸ¥ robots.txt
        try:
            robots_url = urljoin(website_url, '/robots.txt')
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                root_files["has_robots_txt"] = True
                robots_content = response.text.lower()
                
                # æª¢æŸ¥æ˜¯å¦å…è¨± AI bots
                ai_bots = ['google-extended', 'gptbot', 'anthropic-ai', 'claude-ai']
                blocked_ai_bots = []
                for bot in ai_bots:
                    if f'user-agent: {bot}' in robots_content and 'disallow: /' in robots_content:
                        blocked_ai_bots.append(bot)
                
                root_files["robots_allows_ai_bots"] = len(blocked_ai_bots) == 0
                if blocked_ai_bots:
                    st.warning(f"âš ï¸ robots.txt å°é–äº† AI bots: {', '.join(blocked_ai_bots)}")
                else:
                    st.success("âœ… robots.txt å…è¨± AI bots å­˜å–")
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•è®€å– robots.txt: {str(e)}")
        
        # æª¢æŸ¥ sitemap.xml
        try:
            sitemap_url = urljoin(website_url, '/sitemap.xml')
            response = self.session.get(sitemap_url, timeout=10)
            if response.status_code == 200:
                root_files["has_sitemap_xml"] = True
                # ç°¡å–®é©—è­‰ XML æ ¼å¼
                if '<?xml' in response.text and '<urlset' in response.text:
                    root_files["sitemap_is_valid"] = True
                    st.success("âœ… sitemap.xml å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º")
                else:
                    st.warning("âš ï¸ sitemap.xml æ ¼å¼å¯èƒ½æœ‰å•é¡Œ")
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•è®€å– sitemap.xml: {str(e)}")
        
        # æª¢æŸ¥ llms.txt (å‰ç»æ€§æŒ‡æ¨™)
        try:
            llms_url = urljoin(website_url, '/llms.txt')
            response = self.session.get(llms_url, timeout=10)
            if response.status_code == 200:
                root_files["has_llms_txt"] = True
                root_files["llms_txt_content"] = response.text
                st.success("âœ… llms.txt å­˜åœ¨ (å‰ç»æ€§æŒ‡æ¨™)")
        except Exception as e:
            st.info("â„¹ï¸ llms.txt ä¸å­˜åœ¨ (é€™æ˜¯æ­£å¸¸çš„ï¼Œç›®å‰ä»æ˜¯æ–°èˆˆæ¨™æº–)")
        
        return root_files
    
    def _check_architecture_signals(self, website_url: str) -> Dict:
        """æª¢æŸ¥ç¶²ç«™æ¶æ§‹èˆ‡æ¬Šå¨ä¿¡è™Ÿ"""
        st.write("ğŸ—ï¸ æª¢æŸ¥ç¶²ç«™æ¶æ§‹...")
        
        architecture_signals = {
            "uses_https": False,
            "estimated_authority_links": 0,
            "internal_link_structure": "unknown",
            "external_links_count": 0
        }
        
        # æª¢æŸ¥ HTTPS
        if website_url.startswith('https://'):
            architecture_signals["uses_https"] = True
            st.success("âœ… ç¶²ç«™ä½¿ç”¨ HTTPS")
        else:
            st.warning("âš ï¸ ç¶²ç«™æœªä½¿ç”¨ HTTPS")
        
        # æª¢æŸ¥å…§éƒ¨é€£çµçµæ§‹
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥å°èˆªé€£çµ
                nav_links = soup.find_all('a', href=True)
                internal_links = [link for link in nav_links if link['href'].startswith('/') or website_url in link['href']]
                
                if len(internal_links) >= 5:
                    architecture_signals["internal_link_structure"] = "good"
                    st.success("âœ… å…§éƒ¨é€£çµçµæ§‹è‰¯å¥½")
                elif len(internal_links) >= 2:
                    architecture_signals["internal_link_structure"] = "fair"
                    st.info("â„¹ï¸ å…§éƒ¨é€£çµçµæ§‹ä¸€èˆ¬")
                else:
                    architecture_signals["internal_link_structure"] = "poor"
                    st.warning("âš ï¸ å…§éƒ¨é€£çµçµæ§‹è¼ƒå·®")
                
                # ä¼°ç®—å¤–éƒ¨æ¬Šå¨é€£çµ (æ¨¡æ“¬)
                architecture_signals["estimated_authority_links"] = len(nav_links) // 10
                architecture_signals["external_links_count"] = len([link for link in nav_links if not link['href'].startswith('/') and website_url not in link['href']])
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æç¶²ç«™æ¶æ§‹: {str(e)}")
        
        return architecture_signals
    
    def _check_llm_friendliness(self, website_url: str) -> Dict:
        """æª¢æŸ¥ LLM å‹å–„åº¦æŒ‡æ¨™"""
        st.write("ğŸ¤– æª¢æŸ¥ LLM å‹å–„åº¦...")
        
        llm_friendliness = {
            "schema_detected": [],
            "pagespeed_scores": {
                "mobile": {"performance": 0, "lcp": 0, "cls": 0},
                "desktop": {"performance": 0, "lcp": 0, "cls": 0}
            },
            "content_readability": "unknown",
            "structured_data_score": 0,
            "semantic_html": False,
            "content_hierarchy": "unknown"
        }
        
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥ Schema.org çµæ§‹åŒ–è³‡æ–™
                schema_scripts = soup.find_all('script', type='application/ld+json')
                schema_types = []
                for script in schema_scripts:
                    try:
                        schema_data = json.loads(script.string)
                        if isinstance(schema_data, dict):
                            schema_type = schema_data.get('@type', 'Unknown')
                            schema_types.append(schema_type)
                    except:
                        continue
                
                llm_friendliness["schema_detected"] = list(set(schema_types))
                llm_friendliness["structured_data_score"] = len(schema_types)
                
                if schema_types:
                    st.success(f"âœ… ç™¼ç¾çµæ§‹åŒ–è³‡æ–™: {', '.join(schema_types)}")
                else:
                    st.warning("âš ï¸ æœªç™¼ç¾çµæ§‹åŒ–è³‡æ–™")
                
                # æª¢æŸ¥å…§å®¹å¯è®€æ€§
                headings = soup.find_all(['h1', 'h2', 'h3'])
                paragraphs = soup.find_all('p')
                
                if len(headings) >= 3 and len(paragraphs) >= 5:
                    llm_friendliness["content_readability"] = "good"
                    st.success("âœ… å…§å®¹çµæ§‹è‰¯å¥½ï¼Œæœ‰æ¸…æ™°çš„æ¨™é¡Œå±¤ç´š")
                elif len(headings) >= 1 and len(paragraphs) >= 2:
                    llm_friendliness["content_readability"] = "fair"
                    st.info("â„¹ï¸ å…§å®¹çµæ§‹ä¸€èˆ¬")
                else:
                    llm_friendliness["content_readability"] = "poor"
                    st.warning("âš ï¸ å…§å®¹çµæ§‹è¼ƒå·®ï¼Œç¼ºä¹æ¸…æ™°çš„æ¨™é¡Œå±¤ç´š")
                
                # æª¢æŸ¥èªç¾©åŒ– HTML
                semantic_elements = soup.find_all(['article', 'section', 'nav', 'header', 'footer', 'main', 'aside'])
                llm_friendliness["semantic_html"] = len(semantic_elements) > 0
                
                if llm_friendliness["semantic_html"]:
                    st.success("âœ… ä½¿ç”¨èªç¾©åŒ– HTML æ¨™ç±¤")
                else:
                    st.warning("âš ï¸ æœªä½¿ç”¨èªç¾©åŒ– HTML æ¨™ç±¤")
                
                # æª¢æŸ¥å…§å®¹å±¤ç´šçµæ§‹
                h1_count = len(soup.find_all('h1'))
                h2_count = len(soup.find_all('h2'))
                h3_count = len(soup.find_all('h3'))
                
                if h1_count == 1 and h2_count > 0:
                    llm_friendliness["content_hierarchy"] = "good"
                    st.success("âœ… å…§å®¹å±¤ç´šçµæ§‹è‰¯å¥½")
                elif h1_count > 0:
                    llm_friendliness["content_hierarchy"] = "fair"
                    st.info("â„¹ï¸ å…§å®¹å±¤ç´šçµæ§‹ä¸€èˆ¬")
                else:
                    llm_friendliness["content_hierarchy"] = "poor"
                    st.warning("âš ï¸ å…§å®¹å±¤ç´šçµæ§‹è¼ƒå·®")
                
                # æ¨¡æ“¬ PageSpeed åˆ†æ•¸ (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨ Google PageSpeed Insights API)
                llm_friendliness["pagespeed_scores"] = {
                    "mobile": {"performance": 75, "lcp": 2.1, "cls": 0.05},
                    "desktop": {"performance": 92, "lcp": 1.5, "cls": 0.01}
                }
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æ LLM å‹å–„åº¦: {str(e)}")
        
        return llm_friendliness
    
    def _check_product_category_authority(self, website_url: str, product_category: str = None) -> Dict:
        """æª¢æŸ¥ç”¢å“å“é¡æ¬Šå¨æ€§"""
        st.write("ğŸ† æª¢æŸ¥ç”¢å“å“é¡æ¬Šå¨æ€§...")
        
        product_authority = {
            "product_pages_found": 0,
            "product_info_completeness": "unknown",
            "technical_specs_available": False,
            "comparison_features": False,
            "expert_content": False,
            "authority_score": 0
        }
        
        if not product_category:
            st.info("â„¹ï¸ æœªæŒ‡å®šç”¢å“å“é¡ï¼Œè·³éç”¢å“æ¬Šå¨æ€§æª¢æŸ¥")
            return product_authority
        
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æœå°‹ç”¢å“ç›¸é—œé é¢
                product_keywords = [product_category.lower()]
                
                # æ ¹æ“šç”¢å“å“é¡æ·»åŠ ç›¸é—œé—œéµå­—
                if product_category == "é™¤æ¿•æ©Ÿ":
                    product_keywords.extend(["dehumidifier", "é™¤æ¿•", "ä¹¾ç‡¥", "æ¿•åº¦"])
                elif product_category == "å†·æ°£":
                    product_keywords.extend(["air conditioner", "å†·æ°£", "ç©ºèª¿", "è£½å†·"])
                elif product_category == "æ´—è¡£æ©Ÿ":
                    product_keywords.extend(["washing machine", "æ´—è¡£", "æ´—æ»Œ"])
                elif product_category == "å†°ç®±":
                    product_keywords.extend(["refrigerator", "å†°ç®±", "å†·è—", "å†·å‡"])
                elif product_category == "é›»è¦–":
                    product_keywords.extend(["tv", "television", "é›»è¦–", "é¡¯ç¤ºå™¨"])
                elif product_category == "æ‰‹æ©Ÿ":
                    product_keywords.extend(["mobile", "phone", "smartphone", "æ‰‹æ©Ÿ", "æ™ºæ…§å‹æ‰‹æ©Ÿ"])
                elif product_category == "ç­†é›»":
                    product_keywords.extend(["laptop", "notebook", "ç­†é›»", "ç­†è¨˜å‹é›»è…¦"])
                elif product_category == "å¹³æ¿":
                    product_keywords.extend(["tablet", "ipad", "å¹³æ¿", "å¹³æ¿é›»è…¦"])
                elif product_category == "ç›¸æ©Ÿ":
                    product_keywords.extend(["camera", "ç›¸æ©Ÿ", "æ”å½±", "æ‹ç…§"])
                elif product_category == "éŸ³éŸ¿":
                    product_keywords.extend(["speaker", "audio", "éŸ³éŸ¿", "å–‡å­"])
                else:
                    # å°æ–¼å…¶ä»–ç”¢å“ï¼Œæ·»åŠ ä¸€äº›é€šç”¨çš„ç”¢å“ç›¸é—œé—œéµå­—
                    product_keywords.extend(["ç”¢å“", "product", "è¦æ ¼", "specification", "åŠŸèƒ½", "feature"])
                
                # æª¢æŸ¥ç”¢å“é é¢
                product_links = []
                for link in soup.find_all('a', href=True):
                    link_text = link.get_text().lower()
                    href = link['href'].lower()
                    
                    for keyword in product_keywords:
                        if keyword in link_text or keyword in href:
                            product_links.append(link)
                            break
                
                product_authority["product_pages_found"] = len(product_links)
                
                if product_links:
                    st.success(f"âœ… ç™¼ç¾ {len(product_links)} å€‹ç”¢å“ç›¸é—œé é¢")
                    
                    # æª¢æŸ¥ç”¢å“è³‡è¨Šå®Œæ•´æ€§
                    page_text = soup.get_text().lower()
                    
                    # æª¢æŸ¥æŠ€è¡“è¦æ ¼
                    tech_specs_keywords = ["è¦æ ¼", "specification", "æŠ€è¡“", "technical", "åƒæ•¸", "parameter"]
                    if any(keyword in page_text for keyword in tech_specs_keywords):
                        product_authority["technical_specs_available"] = True
                        st.success("âœ… ç™¼ç¾æŠ€è¡“è¦æ ¼è³‡è¨Š")
                    
                    # æª¢æŸ¥æ¯”è¼ƒåŠŸèƒ½
                    comparison_keywords = ["æ¯”è¼ƒ", "compare", "å°æ¯”", "vs", "versus"]
                    if any(keyword in page_text for keyword in comparison_keywords):
                        product_authority["comparison_features"] = True
                        st.success("âœ… ç™¼ç¾ç”¢å“æ¯”è¼ƒåŠŸèƒ½")
                    
                    # æª¢æŸ¥å°ˆå®¶å…§å®¹
                    expert_keywords = ["å°ˆå®¶", "expert", "å°ˆæ¥­", "professional", "è©•æ¸¬", "review"]
                    if any(keyword in page_text for keyword in expert_keywords):
                        product_authority["expert_content"] = True
                        st.success("âœ… ç™¼ç¾å°ˆå®¶å…§å®¹")
                    
                    # è¨ˆç®—æ¬Šå¨åˆ†æ•¸
                    score = 0
                    score += len(product_links) * 10
                    if product_authority["technical_specs_available"]:
                        score += 20
                    if product_authority["comparison_features"]:
                        score += 15
                    if product_authority["expert_content"]:
                        score += 15
                    
                    product_authority["authority_score"] = min(score, 100)
                    
                    if score >= 60:
                        product_authority["product_info_completeness"] = "excellent"
                    elif score >= 40:
                        product_authority["product_info_completeness"] = "good"
                    elif score >= 20:
                        product_authority["product_info_completeness"] = "fair"
                    else:
                        product_authority["product_info_completeness"] = "poor"
                        
                else:
                    st.warning(f"âš ï¸ æœªç™¼ç¾ {product_category} ç›¸é—œç”¢å“é é¢")
                    
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æç”¢å“æ¬Šå¨æ€§: {str(e)}")
        
        return product_authority
    
    def _check_faq_and_consumer_qa(self, website_url: str, product_category: str = None) -> Dict:
        """æª¢æŸ¥ FAQ èˆ‡æ¶ˆè²»è€…å•é¡Œè§£ç­”"""
        st.write("â“ æª¢æŸ¥ FAQ èˆ‡æ¶ˆè²»è€…å•é¡Œè§£ç­”...")
        
        faq_analysis = {
            "faq_section_found": False,
            "faq_count": 0,
            "product_specific_qa": False,
            "common_questions_covered": False,
            "qa_content_quality": "unknown",
            "qa_score": 0
        }
        
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æœå°‹ FAQ ç›¸é—œå…ƒç´ 
                faq_keywords = ["faq", "å¸¸è¦‹å•é¡Œ", "frequently asked", "q&a", "å•ç­”"]
                faq_elements = []
                
                # æª¢æŸ¥æ¨™é¡Œä¸­çš„ FAQ
                for heading in soup.find_all(['h1', 'h2', 'h3', 'h4']):
                    heading_text = heading.get_text().lower()
                    if any(keyword in heading_text for keyword in faq_keywords):
                        faq_elements.append(heading)
                
                # æª¢æŸ¥ FAQ å€å¡Š
                faq_sections = soup.find_all(['div', 'section'], class_=re.compile(r'faq|question|answer', re.I))
                faq_elements.extend(faq_sections)
                
                if faq_elements:
                    faq_analysis["faq_section_found"] = True
                    faq_analysis["faq_count"] = len(faq_elements)
                    st.success(f"âœ… ç™¼ç¾ FAQ å€å¡Šï¼ŒåŒ…å« {len(faq_elements)} å€‹å•é¡Œ")
                    
                    # æª¢æŸ¥ç”¢å“ç‰¹å®šå•é¡Œ
                    if product_category:
                        page_text = soup.get_text().lower()
                        product_keywords = [product_category.lower()]
                        
                        # æ ¹æ“šç”¢å“å“é¡æ·»åŠ ç›¸é—œé—œéµå­—
                        if product_category == "é™¤æ¿•æ©Ÿ":
                            product_keywords.extend(["é™¤æ¿•", "æ¿•åº¦", "ä¹¾ç‡¥", "å†·å‡"])
                        elif product_category == "å†·æ°£":
                            product_keywords.extend(["å†·æ°£", "ç©ºèª¿", "è£½å†·", "æº«åº¦"])
                        elif product_category == "æ´—è¡£æ©Ÿ":
                            product_keywords.extend(["æ´—è¡£", "æ´—æ»Œ", "æ¸…æ½”"])
                        elif product_category == "å†°ç®±":
                            product_keywords.extend(["å†°ç®±", "å†·è—", "å†·å‡", "ä¿é®®"])
                        elif product_category == "é›»è¦–":
                            product_keywords.extend(["é›»è¦–", "é¡¯ç¤ºå™¨", "è¢å¹•"])
                        elif product_category == "æ‰‹æ©Ÿ":
                            product_keywords.extend(["æ‰‹æ©Ÿ", "æ™ºæ…§å‹æ‰‹æ©Ÿ", "é€šè©±", "app"])
                        elif product_category == "ç­†é›»":
                            product_keywords.extend(["ç­†é›»", "ç­†è¨˜å‹é›»è…¦", "é›»è…¦", "è™•ç†å™¨"])
                        elif product_category == "å¹³æ¿":
                            product_keywords.extend(["å¹³æ¿", "å¹³æ¿é›»è…¦", "è§¸æ§"])
                        elif product_category == "ç›¸æ©Ÿ":
                            product_keywords.extend(["ç›¸æ©Ÿ", "æ”å½±", "æ‹ç…§", "é¡é ­"])
                        elif product_category == "éŸ³éŸ¿":
                            product_keywords.extend(["éŸ³éŸ¿", "å–‡å­", "éŸ³æ¨‚", "éŸ³è³ª"])
                        else:
                            # å°æ–¼å…¶ä»–ç”¢å“ï¼Œæ·»åŠ ä¸€äº›é€šç”¨çš„ç”¢å“ç›¸é—œé—œéµå­—
                            product_keywords.extend(["ç”¢å“", "ä½¿ç”¨", "åŠŸèƒ½", "å•é¡Œ"])
                        
                        if any(keyword in page_text for keyword in product_keywords):
                            faq_analysis["product_specific_qa"] = True
                            st.success("âœ… ç™¼ç¾ç”¢å“ç‰¹å®šå•é¡Œè§£ç­”")
                    
                    # æª¢æŸ¥å¸¸è¦‹å•é¡Œè¦†è“‹åº¦
                    common_questions = [
                        "å¦‚ä½•", "æ€éº¼", "ç‚ºä»€éº¼", "ä»€éº¼æ™‚å€™", "å“ªè£¡", "å¤šå°‘éŒ¢",
                        "how", "why", "when", "where", "what", "price", "cost"
                    ]
                    
                    question_count = 0
                    for element in faq_elements:
                        element_text = element.get_text().lower()
                        for question in common_questions:
                            if question in element_text:
                                question_count += 1
                                break
                    
                    if question_count >= 3:
                        faq_analysis["common_questions_covered"] = True
                        st.success("âœ… è¦†è“‹å¤šå€‹å¸¸è¦‹å•é¡Œé¡å‹")
                    
                    # è©•ä¼° QA å…§å®¹å“è³ª
                    score = 0
                    score += len(faq_elements) * 5
                    if faq_analysis["product_specific_qa"]:
                        score += 20
                    if faq_analysis["common_questions_covered"]:
                        score += 15
                    
                    faq_analysis["qa_score"] = min(score, 100)
                    
                    if score >= 50:
                        faq_analysis["qa_content_quality"] = "excellent"
                    elif score >= 30:
                        faq_analysis["qa_content_quality"] = "good"
                    elif score >= 15:
                        faq_analysis["qa_content_quality"] = "fair"
                    else:
                        faq_analysis["qa_content_quality"] = "poor"
                        
                else:
                    st.warning("âš ï¸ æœªç™¼ç¾ FAQ å€å¡Š")
                    
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æ FAQ: {str(e)}")
        
        return faq_analysis
    
    def _generate_recommendations(self, root_files: Dict, architecture_signals: Dict, 
                                llm_friendliness: Dict, product_authority: Dict, faq_analysis: Dict) -> List[Dict]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆæ”¹å–„å»ºè­°"""
        if not self.gemini_model:
            return self._generate_fallback_recommendations(
                root_files, architecture_signals, llm_friendliness, product_authority, faq_analysis
            )
        
        st.write("ğŸ¤– ç”Ÿæˆ AI æ”¹å–„å»ºè­°...")
        
        try:
            # æº–å‚™åˆ†ææ•¸æ“š
            analysis_data = {
                "root_files": root_files,
                "architecture_signals": architecture_signals,
                "llm_friendliness": llm_friendliness,
                "product_authority": product_authority,
                "faq_analysis": faq_analysis
            }
            
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ SIE æŠ€è¡“é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­å„ªåŒ–ç¶²ç«™ä»¥æå‡ AI å°±ç·’åº¦ã€‚

è«‹æ ¹æ“šä»¥ä¸‹ç¶²ç«™åˆ†æçµæœï¼Œæä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æ”¹å–„å»ºè­°ï¼š

åˆ†ææ•¸æ“šï¼š
{json.dumps(analysis_data, indent=2, ensure_ascii=False)}

è«‹ä»¥ JSON æ ¼å¼å›å‚³æ”¹å–„å»ºè­°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "recommendations": [
    {{
      "issue": "å•é¡Œæè¿°",
      "recommendation": "å…·é«”æ”¹å–„å»ºè­°",
      "priority": "High/Medium/Low",
      "category": "Root Files/Architecture/LLM Friendliness/Product Authority/FAQ"
    }}
  ]
}}

è«‹ç¢ºä¿å»ºè­°å…·é«”ã€å¯åŸ·è¡Œï¼Œä¸¦é‡å° AI å°±ç·’åº¦å„ªåŒ–ã€‚
è«‹åªå›å‚³ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
"""
            
            response = self.gemini_model.generate_content(prompt)
            
            # å˜—è©¦è§£æ JSON å›æ‡‰
            try:
                # æ¸…ç†å›æ‡‰æ–‡å­—ï¼Œç§»é™¤å¯èƒ½çš„ markdown æ ¼å¼
                response_text = response.text.strip()
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                response_text = response_text.strip()
                
                recommendations_data = json.loads(response_text)
                return recommendations_data.get("recommendations", [])
                
            except json.JSONDecodeError as json_error:
                st.warning(f"âš ï¸ Gemini API å›æ‡‰æ ¼å¼éŒ¯èª¤: {str(json_error)}")
                st.info("ä½¿ç”¨å‚™ç”¨å»ºè­°ç”Ÿæˆ...")
                return self._generate_fallback_recommendations(
                    root_files, architecture_signals, llm_friendliness, product_authority, faq_analysis
                )
            
        except Exception as e:
            st.warning(f"âš ï¸ Gemini API ç”Ÿæˆå»ºè­°å¤±æ•—: {str(e)}")
            st.info("ä½¿ç”¨å‚™ç”¨å»ºè­°ç”Ÿæˆ...")
            return self._generate_fallback_recommendations(
                root_files, architecture_signals, llm_friendliness, product_authority, faq_analysis
            )
    
    def _generate_fallback_recommendations(self, root_files: Dict, architecture_signals: Dict, 
                                         llm_friendliness: Dict, product_authority: Dict, faq_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚™ç”¨æ”¹å–„å»ºè­°ï¼ˆç•¶ Gemini API ä¸å¯ç”¨æ™‚ï¼‰"""
        recommendations = []
        
        # Root Files å»ºè­°
        if not root_files["has_robots_txt"]:
            recommendations.append({
                "issue": "ç¼ºå°‘ robots.txt æª”æ¡ˆ",
                "recommendation": "è«‹åœ¨ç¶²ç«™æ ¹ç›®éŒ„å»ºç«‹ robots.txt æª”æ¡ˆï¼Œé€™æ˜¯ç¶²ç«™èˆ‡æœå°‹å¼•æ“å’Œ AI æ©Ÿå™¨äººæºé€šçš„é‡è¦æª”æ¡ˆã€‚\n\nå»ºè­°å…§å®¹ï¼š\n```\nUser-agent: *\nAllow: /\nUser-agent: Google-Extended\nAllow: /\nUser-agent: GPTBot\nAllow: /\nUser-agent: anthropic-ai\nAllow: /\n\nSitemap: https://yourdomain.com/sitemap.xml\n```\n\né€™å°‡ç¢ºä¿æ‰€æœ‰æœå°‹å¼•æ“å’Œ AI æ©Ÿå™¨äººéƒ½èƒ½æ­£ç¢ºå­˜å–æ‚¨çš„ç¶²ç«™å…§å®¹ã€‚",
                "priority": "High",
                "category": "Root Files"
            })
        
        if not root_files["robots_allows_ai_bots"]:
            recommendations.append({
                "issue": "robots.txt å°é– AI æ©Ÿå™¨äºº",
                "recommendation": "è«‹ä¿®æ”¹ robots.txtï¼Œç¢ºä¿å…è¨± Google-Extended èˆ‡ GPTBot ç­‰ AI User-Agent é€²è¡Œå­˜å–ã€‚å»ºè­°æ·»åŠ ï¼šUser-agent: Google-Extended å’Œ Allow: /ã€‚",
                "priority": "High",
                "category": "Root Files"
            })
        
        if not root_files["has_sitemap_xml"]:
            recommendations.append({
                "issue": "ç¼ºå°‘ sitemap.xml æª”æ¡ˆ",
                "recommendation": "è«‹å»ºç«‹ sitemap.xml æª”æ¡ˆï¼Œé€™æ˜¯å¹«åŠ©æœå°‹å¼•æ“å’Œ AI ç†è§£ç¶²ç«™çµæ§‹çš„é‡è¦æª”æ¡ˆã€‚\n\nå»ºè­°å…§å®¹çµæ§‹ï¼š\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n  <url>\n    <loc>https://yourdomain.com/</loc>\n    <lastmod>2024-01-01</lastmod>\n    <changefreq>weekly</changefreq>\n    <priority>1.0</priority>\n  </url>\n  <url>\n    <loc>https://yourdomain.com/products</loc>\n    <lastmod>2024-01-01</lastmod>\n    <changefreq>weekly</changefreq>\n    <priority>0.8</priority>\n  </url>\n</urlset>\n```\n\nåŒ…å«æ‰€æœ‰é‡è¦é é¢ï¼Œä¸¦å®šæœŸæ›´æ–°ä»¥åæ˜ æœ€æ–°å…§å®¹ã€‚",
                "priority": "Medium",
                "category": "Root Files"
            })
        
        if not root_files["sitemap_is_valid"]:
            recommendations.append({
                "issue": "sitemap.xml æ ¼å¼éŒ¯èª¤",
                "recommendation": "è«‹æª¢æŸ¥ sitemap.xml æª”æ¡ˆæ ¼å¼ï¼Œç¢ºä¿ç¬¦åˆ XML æ¨™æº–ï¼ŒåŒ…å«æ­£ç¢ºçš„ URL çµæ§‹ã€‚",
                "priority": "Medium",
                "category": "Root Files"
            })
        
        # Architecture å»ºè­°
        if not architecture_signals["uses_https"]:
            recommendations.append({
                "issue": "æœªä½¿ç”¨ HTTPS åŠ å¯†",
                "recommendation": "è«‹å•Ÿç”¨ HTTPS åŠ å¯†é€£ç·šï¼Œé€™å°ç¶²ç«™å®‰å…¨æ€§å’Œæœå°‹å¼•æ“æ’åéƒ½å¾ˆé‡è¦ã€‚å¯ä»¥é€é SSL æ†‘è­‰æä¾›å•†æˆ– CDN æœå‹™å¯¦ç¾ã€‚",
                "priority": "High",
                "category": "Architecture"
            })
        
        if architecture_signals["internal_link_structure"] == "poor":
            recommendations.append({
                "issue": "å…§éƒ¨é€£çµçµæ§‹è¼ƒå·®",
                "recommendation": "è«‹æ”¹å–„ç¶²ç«™å…§éƒ¨é€£çµçµæ§‹ï¼Œç¢ºä¿ä¸»è¦é é¢éƒ½æœ‰æ¸…æ™°çš„å°èˆªé€£çµï¼Œé€™æœ‰åŠ©æ–¼ AI ç†è§£ç¶²ç«™å…§å®¹é—œè¯æ€§ã€‚",
                "priority": "Medium",
                "category": "Architecture"
            })
        
        # LLM Friendliness å»ºè­°
        if not llm_friendliness["schema_detected"]:
            recommendations.append({
                "issue": "ç¼ºå°‘çµæ§‹åŒ–è³‡æ–™",
                "recommendation": "è«‹ç‚ºç¶²ç«™æ·»åŠ  Schema.org çµæ§‹åŒ–è³‡æ–™ï¼Œé€™å° AI ç†è§£å…§å®¹èªç¾©è‡³é—œé‡è¦ã€‚å»ºè­°å¯¦æ–½ä»¥ä¸‹æ¨™è¨˜ï¼š\n\n1. **çµ„ç¹”æ¨™è¨˜ (Organization)**ï¼šåŒ…å«å…¬å¸åç¨±ã€logoã€è¯çµ¡è³‡è¨Š\n2. **ç”¢å“æ¨™è¨˜ (Product)**ï¼šåŒ…å«ç”¢å“åç¨±ã€æè¿°ã€åƒ¹æ ¼ã€è¦æ ¼\n3. **æ–‡ç« æ¨™è¨˜ (Article)**ï¼šåŒ…å«æ¨™é¡Œã€ä½œè€…ã€ç™¼å¸ƒæ—¥æœŸ\n4. **FAQ æ¨™è¨˜ (FAQPage)**ï¼šåŒ…å«å•é¡Œå’Œç­”æ¡ˆ\n5. **éºµåŒ…å±‘æ¨™è¨˜ (BreadcrumbList)**ï¼šé¡¯ç¤ºé é¢å±¤ç´šçµæ§‹\n\nå¯¦æ–½æ–¹å¼ï¼šåœ¨ HTML çš„ <head> å€å¡Šä¸­æ·»åŠ  <script type=\"application/ld+json\"> æ¨™ç±¤ï¼ŒåŒ…å«çµæ§‹åŒ–è³‡æ–™ JSONã€‚é€™å°‡å¤§å¹…æå‡ AI å°ç¶²ç«™å…§å®¹çš„ç†è§£èƒ½åŠ›ã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        if llm_friendliness["content_readability"] == "poor":
            recommendations.append({
                "issue": "å…§å®¹çµæ§‹è¼ƒå·®",
                "recommendation": "è«‹æ”¹å–„å…§å®¹çµæ§‹ï¼Œä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå±¤ç´šï¼ˆH1, H2, H3ï¼‰çµ„ç¹”å…§å®¹ï¼Œç¢ºä¿å…§å®¹é‚è¼¯æ¸…æ™°ï¼Œä¾¿æ–¼ AI ç†è§£ã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        if not llm_friendliness["semantic_html"]:
            recommendations.append({
                "issue": "æœªä½¿ç”¨èªç¾©åŒ– HTML",
                "recommendation": "è«‹ä½¿ç”¨èªç¾©åŒ– HTML æ¨™ç±¤ï¼Œé€™å° AI ç†è§£å…§å®¹çµæ§‹è‡³é—œé‡è¦ã€‚\n\nå»ºè­°ä½¿ç”¨çš„æ¨™ç±¤ï¼š\n- **<header>**ï¼šé é¢æˆ–å€å¡Šçš„æ¨™é¡Œå€åŸŸ\n- **<nav>**ï¼šå°èˆªé¸å–®\n- **<main>**ï¼šä¸»è¦å…§å®¹å€åŸŸ\n- **<article>**ï¼šç¨ç«‹çš„æ–‡ç« æˆ–ç”¢å“å…§å®¹\n- **<section>**ï¼šå…§å®¹å€å¡Š\n- **<aside>**ï¼šå´é‚Šæ¬„æˆ–ç›¸é—œå…§å®¹\n- **<footer>**ï¼šé é¢åº•éƒ¨\n\nç¯„ä¾‹çµæ§‹ï¼š\n```html\n<header>\n  <nav>å°èˆªé¸å–®</nav>\n</header>\n<main>\n  <article>\n    <section>ç”¢å“ä»‹ç´¹</section>\n    <section>æŠ€è¡“è¦æ ¼</section>\n  </article>\n  <aside>ç›¸é—œç”¢å“</aside>\n</main>\n<footer>è¯çµ¡è³‡è¨Š</footer>\n```\n\né€™å°‡å¤§å¹…æå‡ AI å°ç¶²ç«™çµæ§‹çš„ç†è§£èƒ½åŠ›ã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        if llm_friendliness["content_hierarchy"] == "poor":
            recommendations.append({
                "issue": "å…§å®¹å±¤ç´šçµæ§‹è¼ƒå·®",
                "recommendation": "è«‹æ”¹å–„å…§å®¹å±¤ç´šçµæ§‹ï¼Œç¢ºä¿æ¯å€‹é é¢æœ‰ä¸”åƒ…æœ‰ä¸€å€‹ H1 æ¨™é¡Œï¼Œä¸¦ä½¿ç”¨ H2ã€H3 ç­‰å»ºç«‹æ¸…æ™°çš„å…§å®¹å±¤ç´šã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        # Product Authority å»ºè­°
        if product_authority["product_info_completeness"] in ["poor", "fair"]:
            recommendations.append({
                "issue": "ç”¢å“è³‡è¨Šä¸å®Œæ•´",
                "recommendation": "è«‹å®Œå–„ç”¢å“è³‡è¨Šï¼ŒåŒ…å«è©³ç´°çš„æŠ€è¡“è¦æ ¼ã€ç”¢å“æ¯”è¼ƒåŠŸèƒ½ã€å°ˆå®¶è©•æ¸¬ã€ä½¿ç”¨æŒ‡å—ç­‰å…§å®¹ï¼Œæå‡ç”¢å“æ¬Šå¨æ€§ã€‚",
                "priority": "Medium",
                "category": "Product Authority"
            })
        
        if product_authority["product_pages_found"] == 0:
            recommendations.append({
                "issue": "æœªç™¼ç¾ç”¢å“ç›¸é—œé é¢",
                "recommendation": "è«‹å»ºç«‹å°ˆé–€çš„ç”¢å“é é¢ï¼ŒåŒ…å«ç”¢å“ä»‹ç´¹ã€è¦æ ¼ã€åŠŸèƒ½èªªæ˜ç­‰å…§å®¹ï¼Œå¹«åŠ©æ¶ˆè²»è€…äº†è§£ç”¢å“ç‰¹æ€§ã€‚",
                "priority": "High",
                "category": "Product Authority"
            })
        
        if not product_authority["technical_specs_available"]:
            recommendations.append({
                "issue": "ç¼ºå°‘æŠ€è¡“è¦æ ¼è³‡è¨Š",
                "recommendation": "è«‹ç‚ºç”¢å“æä¾›è©³ç´°çš„æŠ€è¡“è¦æ ¼å’Œåƒæ•¸ï¼ŒåŒ…æ‹¬å°ºå¯¸ã€é‡é‡ã€åŠŸç‡ã€åŠŸèƒ½ç‰¹é»ç­‰ï¼Œæå‡ç”¢å“è³‡è¨Šçš„å°ˆæ¥­æ€§ã€‚",
                "priority": "Medium",
                "category": "Product Authority"
            })
        
        # FAQ å»ºè­°
        if not faq_analysis["faq_section_found"]:
            recommendations.append({
                "issue": "ç¼ºå°‘ FAQ å€å¡Š",
                "recommendation": "è«‹å»ºç«‹ FAQ å€å¡Šï¼Œå›ç­”æ¶ˆè²»è€…å¸¸è¦‹å•é¡Œï¼Œé€™ä¸åƒ…èƒ½æå‡ç”¨æˆ¶é«”é©—ï¼Œä¹Ÿèƒ½å¢åŠ ç¶²ç«™å…§å®¹çš„æ¬Šå¨æ€§ã€‚",
                "priority": "Medium",
                "category": "FAQ"
            })
        
        if faq_analysis["qa_content_quality"] in ["poor", "fair"]:
            recommendations.append({
                "issue": "FAQ å…§å®¹å“è³ªè¼ƒå·®",
                "recommendation": "è«‹æ”¹å–„ FAQ å…§å®¹å“è³ªï¼ŒåŒ…å«ç”¢å“ç‰¹å®šå•é¡Œã€ä½¿ç”¨å•é¡Œã€å¸¸è¦‹å•é¡Œç­‰ï¼Œç¢ºä¿å›ç­”è©³ç´°ä¸”å¯¦ç”¨ã€‚",
                "priority": "Medium",
                "category": "FAQ"
            })
        
        if not faq_analysis["product_specific_qa"]:
            recommendations.append({
                "issue": "ç¼ºå°‘ç”¢å“ç‰¹å®šå•é¡Œè§£ç­”",
                "recommendation": "è«‹åœ¨ FAQ ä¸­åŒ…å«ç”¢å“ç‰¹å®šçš„å•é¡Œå’Œè§£ç­”ï¼Œå¹«åŠ©æ¶ˆè²»è€…æ›´å¥½åœ°äº†è§£ç”¢å“ä½¿ç”¨æ–¹æ³•å’Œæ³¨æ„äº‹é …ã€‚",
                "priority": "Medium",
                "category": "FAQ"
            })
        
        # å¦‚æœæ²’æœ‰ç™¼ç¾ä»»ä½•å•é¡Œï¼Œæä¾›ä¸€èˆ¬æ€§å»ºè­°
        if not recommendations:
            recommendations.append({
                "issue": "ç¶²ç«™åŸºç¤è‰¯å¥½",
                "recommendation": "æ‚¨çš„ç¶²ç«™åŸºç¤æ¶æ§‹è‰¯å¥½ï¼å»ºè­°æŒçºŒç›£æ§ AI å°±ç·’åº¦æŒ‡æ¨™ï¼Œä¸¦è€ƒæ…®å»ºç«‹ llms.txt æª”æ¡ˆä»¥é©æ‡‰æœªä¾† AI æœå°‹éœ€æ±‚ã€‚",
                "priority": "Low",
                "category": "General"
            })
        
        return recommendations
    
    def _generate_seo_llm_recommendations(self, website_url: str, root_files: Dict, architecture_signals: Dict,
                                        llm_friendliness: Dict, product_authority: Dict, faq_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆ SEO èˆ‡ LLM å‹å–„åº¦æ”¹å–„å»ºè­°"""
        st.write("ğŸ¯ ç”Ÿæˆ SEO èˆ‡ LLM å‹å–„åº¦å»ºè­°...")
        
        seo_llm_recommendations = []
        
        # SEO åŸºç¤å»ºè­°
        seo_llm_recommendations.append({
            "category": "SEO åŸºç¤å„ªåŒ–",
            "recommendations": [
                "å»ºç«‹å®Œæ•´çš„ XML Sitemapï¼ŒåŒ…å«æ‰€æœ‰é‡è¦é é¢",
                "å„ªåŒ– robots.txtï¼Œç¢ºä¿æœå°‹å¼•æ“å’Œ AI æ©Ÿå™¨äººæ­£ç¢ºå­˜å–",
                "å¯¦æ–½ HTTPS åŠ å¯†ï¼Œæå‡å®‰å…¨æ€§å’Œä¿¡ä»»åº¦",
                "æ”¹å–„ç¶²ç«™è¼‰å…¥é€Ÿåº¦ï¼Œå„ªåŒ– Core Web Vitals æŒ‡æ¨™"
            ]
        })
        
        # å…§å®¹çµæ§‹å»ºè­°
        content_recommendations = []
        if llm_friendliness["content_readability"] != "good":
            content_recommendations.append("ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå±¤ç´šçµæ§‹ï¼ˆH1 > H2 > H3ï¼‰")
        if not llm_friendliness["semantic_html"]:
            content_recommendations.append("å¯¦æ–½èªç¾©åŒ– HTML æ¨™ç±¤")
        if not llm_friendliness["schema_detected"]:
            content_recommendations.append("æ·»åŠ  Schema.org çµæ§‹åŒ–è³‡æ–™")
        
        if content_recommendations:
            seo_llm_recommendations.append({
                "category": "å…§å®¹çµæ§‹å„ªåŒ–",
                "recommendations": content_recommendations
            })
        
        # LLM å‹å–„åº¦å»ºè­°
        llm_recommendations = [
            "å»ºç«‹ llms.txt æª”æ¡ˆï¼Œæ˜ç¢ºå‘ŠçŸ¥ AI æ¨¡å‹å¦‚ä½•è™•ç†ç¶²ç«™å…§å®¹",
            "ä½¿ç”¨è‡ªç„¶èªè¨€æ’°å¯«å…§å®¹ï¼Œé¿å…éåº¦å„ªåŒ–é—œéµå­—",
            "æä¾›å®Œæ•´çš„ç”¢å“è³‡è¨Šå’ŒæŠ€è¡“è¦æ ¼",
            "å»ºç«‹ FAQ å€å¡Šï¼Œå›ç­”æ¶ˆè²»è€…å¸¸è¦‹å•é¡Œ",
            "ä½¿ç”¨å…§éƒ¨é€£çµå»ºç«‹å…§å®¹é—œè¯æ€§",
            "ç¢ºä¿å…§å®¹çš„å¯è®€æ€§å’Œå¯ç†è§£æ€§"
        ]
        
        seo_llm_recommendations.append({
            "category": "LLM å‹å–„åº¦å„ªåŒ–",
            "recommendations": llm_recommendations
        })
        
        # ç”¢å“æ¬Šå¨æ€§å»ºè­°
        if product_authority["product_info_completeness"] in ["poor", "fair"]:
            seo_llm_recommendations.append({
                "category": "ç”¢å“æ¬Šå¨æ€§å»ºç«‹",
                "recommendations": [
                    "æä¾›è©³ç´°çš„ç”¢å“æŠ€è¡“è¦æ ¼å’Œåƒæ•¸",
                    "å»ºç«‹ç”¢å“æ¯”è¼ƒåŠŸèƒ½ï¼Œå¹«åŠ©æ¶ˆè²»è€…é¸æ“‡",
                    "ç™¼å¸ƒå°ˆå®¶è©•æ¸¬å’Œä½¿ç”¨æŒ‡å—",
                    "å»ºç«‹ç”¢å“ä½¿ç”¨æ•™å­¸å’Œç¶­è­·æŒ‡å—",
                    "æä¾›ç”¢å“ç›¸é—œçš„å°ˆæ¥­çŸ¥è­˜å…§å®¹"
                ]
            })
        
        # æœªä¾† LLM æ”¶éŒ„å»ºè­°
        seo_llm_recommendations.append({
            "category": "æœªä¾† LLM æ”¶éŒ„æº–å‚™",
            "recommendations": [
                "å»ºç«‹å®Œæ•´çš„ç”¢å“çŸ¥è­˜åº«",
                "æä¾›çµæ§‹åŒ–çš„ç”¢å“è³‡è¨Š",
                "ä½¿ç”¨æ¨™æº–åŒ–çš„å…§å®¹æ ¼å¼",
                "å»ºç«‹å…§å®¹æ›´æ–°æ©Ÿåˆ¶",
                "ç›£æ§ AI æ¨¡å‹å°å…§å®¹çš„å­˜å–å’Œä½¿ç”¨æƒ…æ³",
                "æº–å‚™é©æ‡‰æœªä¾† AI æœå°‹æ¼”ç®—æ³•çš„å…§å®¹ç­–ç•¥"
            ]
        })
        
        return seo_llm_recommendations

def run_website_analysis(website_url: str, product_category: Optional[str] = None, gemini_api_key: Optional[str] = None) -> Dict:
    """åŸ·è¡Œç¶²ç«™ AI å°±ç·’åº¦åˆ†æçš„ä¸»å‡½å¼"""
    analyzer = WebsiteAIReadinessAnalyzer(gemini_api_key)
    return analyzer.analyze_website(website_url, product_category) 