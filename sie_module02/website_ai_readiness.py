import requests
import json
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os
from typing import Dict, List, Optional, Tuple
import streamlit as st

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: google-generativeai æœªå®‰è£ï¼ŒGemini API åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

class WebsiteAIReadinessAnalyzer:
    """ç¶²ç«™ AI å°±ç·’åº¦èˆ‡æŠ€è¡“å¥åº·åº¦åˆ†æå™¨"""
    
    def __init__(self, gemini_api_key: Optional[str] = None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SIE-Diagnostic-Tool/1.0 (contact@example.com)'
        })
        
        # åˆå§‹åŒ– Gemini API
        if gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        else:
            self.gemini_model = None
    
    def analyze_website(self, website_url: str) -> Dict:
        """åˆ†æç¶²ç«™çš„ AI å°±ç·’åº¦èˆ‡æŠ€è¡“å¥åº·åº¦"""
        if not website_url.startswith(('http://', 'https://')):
            website_url = 'https://' + website_url
        
        st.info(f"ğŸ” æ­£åœ¨åˆ†æç¶²ç«™: {website_url}")
        
        try:
            # 1. æª¢æŸ¥æ ¹æª”æ¡ˆèˆ‡ LLM éµå¾æ€§
            root_files = self._check_root_files(website_url)
            
            # 2. æª¢æŸ¥ç¶²ç«™æ¶æ§‹èˆ‡æ¬Šå¨ä¿¡è™Ÿ
            architecture_signals = self._check_architecture_signals(website_url)
            
            # 3. æª¢æŸ¥ LLM å‹å–„åº¦æŒ‡æ¨™
            llm_friendliness = self._check_llm_friendliness(website_url)
            
            # 4. ç”Ÿæˆ AI æ”¹å–„å»ºè­°
            actionable_recommendations = self._generate_recommendations(
                root_files, architecture_signals, llm_friendliness
            )
            
            return {
                "technical_seo_ai_readiness": {
                    "root_files": root_files,
                    "architecture_signals": architecture_signals,
                    "llm_friendliness": llm_friendliness,
                    "actionable_recommendations": actionable_recommendations
                }
            }
            
        except Exception as e:
            st.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {"error": str(e)}
    
    def _check_root_files(self, website_url: str) -> Dict:
        """æª¢æŸ¥æ ¹æª”æ¡ˆèˆ‡ LLM éµå¾æ€§"""
        st.write("ğŸ“ æª¢æŸ¥æ ¹æª”æ¡ˆ...")
        
        root_files = {
            "has_robots_txt": False,
            "robots_allows_ai_bots": False,
            "has_sitemap_xml": False,
            "sitemap_is_valid": False,
            "has_llms_txt": False,
            "llms_txt_content": None
        }
        
        # æª¢æŸ¥ robots.txt
        try:
            robots_url = urljoin(website_url, '/robots.txt')
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                root_files["has_robots_txt"] = True
                robots_content = response.text.lower()
                
                # æª¢æŸ¥æ˜¯å¦å…è¨± AI bots
                ai_bots = ['google-extended', 'gptbot', 'anthropic-ai', 'claude-ai']
                blocked_ai_bots = []
                for bot in ai_bots:
                    if f'user-agent: {bot}' in robots_content and 'disallow: /' in robots_content:
                        blocked_ai_bots.append(bot)
                
                root_files["robots_allows_ai_bots"] = len(blocked_ai_bots) == 0
                if blocked_ai_bots:
                    st.warning(f"âš ï¸ robots.txt å°é–äº† AI bots: {', '.join(blocked_ai_bots)}")
                else:
                    st.success("âœ… robots.txt å…è¨± AI bots å­˜å–")
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•è®€å– robots.txt: {str(e)}")
        
        # æª¢æŸ¥ sitemap.xml
        try:
            sitemap_url = urljoin(website_url, '/sitemap.xml')
            response = self.session.get(sitemap_url, timeout=10)
            if response.status_code == 200:
                root_files["has_sitemap_xml"] = True
                # ç°¡å–®é©—è­‰ XML æ ¼å¼
                if '<?xml' in response.text and '<urlset' in response.text:
                    root_files["sitemap_is_valid"] = True
                    st.success("âœ… sitemap.xml å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º")
                else:
                    st.warning("âš ï¸ sitemap.xml æ ¼å¼å¯èƒ½æœ‰å•é¡Œ")
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•è®€å– sitemap.xml: {str(e)}")
        
        # æª¢æŸ¥ llms.txt (å‰ç»æ€§æŒ‡æ¨™)
        try:
            llms_url = urljoin(website_url, '/llms.txt')
            response = self.session.get(llms_url, timeout=10)
            if response.status_code == 200:
                root_files["has_llms_txt"] = True
                root_files["llms_txt_content"] = response.text
                st.success("âœ… llms.txt å­˜åœ¨ (å‰ç»æ€§æŒ‡æ¨™)")
        except Exception as e:
            st.info("â„¹ï¸ llms.txt ä¸å­˜åœ¨ (é€™æ˜¯æ­£å¸¸çš„ï¼Œç›®å‰ä»æ˜¯æ–°èˆˆæ¨™æº–)")
        
        return root_files
    
    def _check_architecture_signals(self, website_url: str) -> Dict:
        """æª¢æŸ¥ç¶²ç«™æ¶æ§‹èˆ‡æ¬Šå¨ä¿¡è™Ÿ"""
        st.write("ğŸ—ï¸ æª¢æŸ¥ç¶²ç«™æ¶æ§‹...")
        
        architecture_signals = {
            "uses_https": False,
            "estimated_authority_links": 0,
            "internal_link_structure": "unknown",
            "external_links_count": 0
        }
        
        # æª¢æŸ¥ HTTPS
        if website_url.startswith('https://'):
            architecture_signals["uses_https"] = True
            st.success("âœ… ç¶²ç«™ä½¿ç”¨ HTTPS")
        else:
            st.warning("âš ï¸ ç¶²ç«™æœªä½¿ç”¨ HTTPS")
        
        # æª¢æŸ¥å…§éƒ¨é€£çµçµæ§‹
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥å°èˆªé€£çµ
                nav_links = soup.find_all('a', href=True)
                internal_links = [link for link in nav_links if link['href'].startswith('/') or website_url in link['href']]
                
                if len(internal_links) >= 5:
                    architecture_signals["internal_link_structure"] = "good"
                    st.success("âœ… å…§éƒ¨é€£çµçµæ§‹è‰¯å¥½")
                elif len(internal_links) >= 2:
                    architecture_signals["internal_link_structure"] = "fair"
                    st.info("â„¹ï¸ å…§éƒ¨é€£çµçµæ§‹ä¸€èˆ¬")
                else:
                    architecture_signals["internal_link_structure"] = "poor"
                    st.warning("âš ï¸ å…§éƒ¨é€£çµçµæ§‹è¼ƒå·®")
                
                # ä¼°ç®—å¤–éƒ¨æ¬Šå¨é€£çµ (æ¨¡æ“¬)
                architecture_signals["estimated_authority_links"] = len(nav_links) // 10
                architecture_signals["external_links_count"] = len([link for link in nav_links if not link['href'].startswith('/') and website_url not in link['href']])
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æç¶²ç«™æ¶æ§‹: {str(e)}")
        
        return architecture_signals
    
    def _check_llm_friendliness(self, website_url: str) -> Dict:
        """æª¢æŸ¥ LLM å‹å–„åº¦æŒ‡æ¨™"""
        st.write("ğŸ¤– æª¢æŸ¥ LLM å‹å–„åº¦...")
        
        llm_friendliness = {
            "schema_detected": [],
            "pagespeed_scores": {
                "mobile": {"performance": 0, "lcp": 0, "cls": 0},
                "desktop": {"performance": 0, "lcp": 0, "cls": 0}
            },
            "content_readability": "unknown",
            "structured_data_score": 0
        }
        
        try:
            response = self.session.get(website_url, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥ Schema.org çµæ§‹åŒ–è³‡æ–™
                schema_scripts = soup.find_all('script', type='application/ld+json')
                schema_types = []
                for script in schema_scripts:
                    try:
                        schema_data = json.loads(script.string)
                        if isinstance(schema_data, dict):
                            schema_type = schema_data.get('@type', 'Unknown')
                            schema_types.append(schema_type)
                    except:
                        continue
                
                llm_friendliness["schema_detected"] = list(set(schema_types))
                llm_friendliness["structured_data_score"] = len(schema_types)
                
                if schema_types:
                    st.success(f"âœ… ç™¼ç¾çµæ§‹åŒ–è³‡æ–™: {', '.join(schema_types)}")
                else:
                    st.warning("âš ï¸ æœªç™¼ç¾çµæ§‹åŒ–è³‡æ–™")
                
                # æª¢æŸ¥å…§å®¹å¯è®€æ€§
                headings = soup.find_all(['h1', 'h2', 'h3'])
                paragraphs = soup.find_all('p')
                
                if len(headings) >= 3 and len(paragraphs) >= 5:
                    llm_friendliness["content_readability"] = "good"
                    st.success("âœ… å…§å®¹çµæ§‹è‰¯å¥½ï¼Œæœ‰æ¸…æ™°çš„æ¨™é¡Œå±¤ç´š")
                elif len(headings) >= 1 and len(paragraphs) >= 2:
                    llm_friendliness["content_readability"] = "fair"
                    st.info("â„¹ï¸ å…§å®¹çµæ§‹ä¸€èˆ¬")
                else:
                    llm_friendliness["content_readability"] = "poor"
                    st.warning("âš ï¸ å…§å®¹çµæ§‹è¼ƒå·®ï¼Œç¼ºä¹æ¸…æ™°çš„æ¨™é¡Œå±¤ç´š")
                
                # æ¨¡æ“¬ PageSpeed åˆ†æ•¸ (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨ Google PageSpeed Insights API)
                llm_friendliness["pagespeed_scores"] = {
                    "mobile": {"performance": 75, "lcp": 2.1, "cls": 0.05},
                    "desktop": {"performance": 92, "lcp": 1.5, "cls": 0.01}
                }
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æ LLM å‹å–„åº¦: {str(e)}")
        
        return llm_friendliness
    
    def _generate_recommendations(self, root_files: Dict, architecture_signals: Dict, llm_friendliness: Dict) -> List[Dict]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆæ”¹å–„å»ºè­°"""
        if not self.gemini_model:
            return self._generate_fallback_recommendations(root_files, architecture_signals, llm_friendliness)
        
        st.write("ğŸ¤– ç”Ÿæˆ AI æ”¹å–„å»ºè­°...")
        
        try:
            # æº–å‚™åˆ†ææ•¸æ“š
            analysis_data = {
                "root_files": root_files,
                "architecture_signals": architecture_signals,
                "llm_friendliness": llm_friendliness
            }
            
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ SIE æŠ€è¡“é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­å„ªåŒ–ç¶²ç«™ä»¥æå‡ AI å°±ç·’åº¦ã€‚

è«‹æ ¹æ“šä»¥ä¸‹ç¶²ç«™åˆ†æçµæœï¼Œæä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æ”¹å–„å»ºè­°ï¼š

åˆ†ææ•¸æ“šï¼š
{json.dumps(analysis_data, indent=2, ensure_ascii=False)}

è«‹ä»¥ JSON æ ¼å¼å›å‚³æ”¹å–„å»ºè­°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "recommendations": [
    {{
      "issue": "å•é¡Œæè¿°",
      "recommendation": "å…·é«”æ”¹å–„å»ºè­°",
      "priority": "High/Medium/Low",
      "category": "Root Files/Architecture/LLM Friendliness"
    }}
  ]
}}

è«‹ç¢ºä¿å»ºè­°å…·é«”ã€å¯åŸ·è¡Œï¼Œä¸¦é‡å° AI å°±ç·’åº¦å„ªåŒ–ã€‚
"""
            
            response = self.gemini_model.generate_content(prompt)
            recommendations_data = json.loads(response.text)
            return recommendations_data.get("recommendations", [])
            
        except Exception as e:
            st.warning(f"âš ï¸ Gemini API ç”Ÿæˆå»ºè­°å¤±æ•—: {str(e)}")
            return self._generate_fallback_recommendations(root_files, architecture_signals, llm_friendliness)
    
    def _generate_fallback_recommendations(self, root_files: Dict, architecture_signals: Dict, llm_friendliness: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚™ç”¨æ”¹å–„å»ºè­°ï¼ˆç•¶ Gemini API ä¸å¯ç”¨æ™‚ï¼‰"""
        recommendations = []
        
        # Root Files å»ºè­°
        if not root_files["has_robots_txt"]:
            recommendations.append({
                "issue": "Missing robots.txt",
                "recommendation": "è«‹åœ¨ç¶²ç«™æ ¹ç›®éŒ„å»ºç«‹ robots.txt æª”æ¡ˆï¼Œä»¥æ­£ç¢ºå¼•å°æœå°‹å¼•æ“å’Œ AI æ©Ÿå™¨äººã€‚",
                "priority": "High",
                "category": "Root Files"
            })
        
        if not root_files["robots_allows_ai_bots"]:
            recommendations.append({
                "issue": "AI Bot Crawling Blocked",
                "recommendation": "è«‹ä¿®æ”¹ robots.txtï¼Œç¢ºä¿å…è¨± Google-Extended èˆ‡ GPTBot ç­‰ AI User-Agent é€²è¡Œå­˜å–ã€‚",
                "priority": "High",
                "category": "Root Files"
            })
        
        if not root_files["has_sitemap_xml"]:
            recommendations.append({
                "issue": "Missing sitemap.xml",
                "recommendation": "è«‹å»ºç«‹ sitemap.xml æª”æ¡ˆï¼Œå¹«åŠ©æœå°‹å¼•æ“å’Œ AI æ›´å¥½åœ°ç†è§£ç¶²ç«™çµæ§‹ã€‚",
                "priority": "Medium",
                "category": "Root Files"
            })
        
        # Architecture å»ºè­°
        if not architecture_signals["uses_https"]:
            recommendations.append({
                "issue": "No HTTPS",
                "recommendation": "è«‹å•Ÿç”¨ HTTPS åŠ å¯†é€£ç·šï¼Œæå‡ç¶²ç«™å®‰å…¨æ€§å’Œä¿¡ä»»åº¦ã€‚",
                "priority": "High",
                "category": "Architecture"
            })
        
        if architecture_signals["internal_link_structure"] == "poor":
            recommendations.append({
                "issue": "Poor Internal Link Structure",
                "recommendation": "è«‹æ”¹å–„ç¶²ç«™å…§éƒ¨é€£çµçµæ§‹ï¼Œç¢ºä¿ä¸»è¦é é¢éƒ½æœ‰æ¸…æ™°çš„å°èˆªé€£çµã€‚",
                "priority": "Medium",
                "category": "Architecture"
            })
        
        # LLM Friendliness å»ºè­°
        if not llm_friendliness["schema_detected"]:
            recommendations.append({
                "issue": "Missing Structured Data",
                "recommendation": "è«‹ç‚ºç¶²ç«™æ·»åŠ  Schema.org çµæ§‹åŒ–è³‡æ–™ï¼Œå¹«åŠ© AI æ›´å¥½åœ°ç†è§£å…§å®¹ã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        if llm_friendliness["content_readability"] == "poor":
            recommendations.append({
                "issue": "Poor Content Structure",
                "recommendation": "è«‹æ”¹å–„å…§å®¹çµæ§‹ï¼Œä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå±¤ç´šï¼ˆH1, H2, H3ï¼‰çµ„ç¹”å…§å®¹ã€‚",
                "priority": "Medium",
                "category": "LLM Friendliness"
            })
        
        return recommendations

def run_website_analysis(website_url: str, gemini_api_key: Optional[str] = None) -> Dict:
    """åŸ·è¡Œç¶²ç«™ AI å°±ç·’åº¦åˆ†æçš„ä¸»å‡½å¼"""
    analyzer = WebsiteAIReadinessAnalyzer(gemini_api_key)
    return analyzer.analyze_website(website_url) 