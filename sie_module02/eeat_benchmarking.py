import requests
import json
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os
from typing import Dict, List, Optional, Tuple
import streamlit as st
from datetime import datetime, timedelta
import random

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: google-generativeai æœªå®‰è£ï¼ŒGemini API åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

class EEATBenchmarkingAnalyzer:
    """å‹•æ…‹ E-E-A-T è©•ä¼°èˆ‡ç«¶çˆ­åŸºæº–åˆ†æå™¨"""
    
    def __init__(self, gemini_api_key: Optional[str] = None, market: Optional[str] = None, product_category: Optional[str] = None, brand: Optional[str] = None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SIE-Benchmarking-Tool/1.0 (contact@example.com)'
        })
        
        # åˆå§‹åŒ– Gemini API
        if gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        else:
            self.gemini_model = None
        
        self.market = market or "å°ç£"
        self.product_category = product_category or ""
        self.brand = brand or ""
        # å‹•æ…‹ç”¢ç”Ÿä¸»æµåª’é«”/è«–å£‡/ç¤¾ç¾¤åå–®
        self.market_media_dict = self._generate_market_media_with_llm(self.market, self.product_category, self.brand)
        # é ç•™ï¼šæ ¹æ“šå¸‚å ´åˆ†é¡çš„ç«¶çˆ­å°æ‰‹æ¸…å–®
        self.market_competitors_dict = {
            "å°ç£": ["å°ç©é›»", "è¯é›»", "ä¸–ç•Œå…ˆé€²"],
            "å…¨çƒ": ["Intel", "Samsung", "TSMC"]
            # ...å¯æ“´å……æ›´å¤šå¸‚å ´
        }
    
    def analyze_eeat_benchmarking(self, target_website: str, competitors: List[str] = []) -> Dict:
        """åŸ·è¡Œå‹•æ…‹ E-E-A-T è©•ä¼°èˆ‡ç«¶çˆ­åŸºæº–åˆ†æ"""
        if not target_website.startswith(('http://', 'https://')):
            target_website = 'https://' + target_website
        st.info(f"ğŸ” æ­£åœ¨åˆ†æç›®æ¨™ç¶²ç«™: {target_website}ï¼ˆå¸‚å ´ï¼š{self.market}ï¼‰")
        try:
            # 0. LLM è¡Œæ¥­/å¸‚å ´/ç”¢å“é ˜å°è€…æ¨è–¦
            leaders_recommendation = self._llm_recommend_leaders(self.market, self.product_category, self.brand, target_website)
            # 0.1 LLM è‡ªå‹•æ¯”å°æœ¬å“ç‰Œèˆ‡æ¨™ç«¿å·®ç•°
            brand_gap_analysis = self._llm_compare_with_benchmarks(self.brand, target_website, self.market, self.product_category, leaders_recommendation)
            # 1. AI é ˜å°è€…è­˜åˆ¥èˆ‡åˆ†æï¼ˆä¿ç•™æŠ€è¡“æŒ‡æ¨™åˆ†æï¼‰
            ai_leader_analysis = self._identify_ai_leaders(target_website)
            # 2. å‹•æ…‹åª’é«”æ¬Šé‡è©•ä¼°
            dynamic_media_weights = self._analyze_dynamic_media_weights(target_website)
            # 3. çœŸå¯¦åª’é«”æ›å…‰ç´€éŒ„æŸ¥è©¢
            real_media_mentions = self._fetch_real_media_mentions(self.market_media_dict)
            # 4. ç«¶çˆ­å°æ‰‹åŸºæº–åˆ†æ
            if not competitors:
                competitors = self.market_competitors_dict.get(self.market, [])
            competitor_benchmarking = self._analyze_competitor_benchmarks(target_website, competitors)
            # 5. è¶¨å‹¢è¿½è¹¤èˆ‡é æ¸¬
            trend_analysis = self._analyze_trends_and_predictions(target_website)
            # 6. ç”Ÿæˆç­–ç•¥å»ºè­°
            strategic_recommendations = self._generate_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
            return {
                "eeat_benchmarking": {
                    "leaders_recommendation": leaders_recommendation,
                    "brand_gap_analysis": brand_gap_analysis,
                    "ai_leader_analysis": ai_leader_analysis,
                    "dynamic_media_weights": dynamic_media_weights,
                    "real_media_mentions": real_media_mentions,
                    "competitor_benchmarking": competitor_benchmarking,
                    "trend_analysis": trend_analysis,
                    "strategic_recommendations": strategic_recommendations,
                    "market": self.market,
                    "market_media": self.market_media_dict,
                    "market_competitors": self.market_competitors_dict.get(self.market, [])
                }
            }
        except Exception as e:
            st.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {"error": str(e)}
    
    def _identify_ai_leaders(self, target_website: str) -> Dict:
        """è­˜åˆ¥èˆ‡åˆ†æ AI é ˜å°è€…"""
        st.write("ğŸ¤– è­˜åˆ¥ AI é ˜å°è€…...")
        
        ai_leader_analysis = {
            "ai_leader_score": 0,
            "ai_technology_indicators": [],
            "ai_content_signals": [],
            "ai_engagement_metrics": {},
            "ai_leadership_position": "unknown"
        }
        
        try:
            response = self.session.get(target_website, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥ AI æŠ€è¡“æŒ‡æ¨™
                ai_tech_indicators = []
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„ meta æ¨™ç±¤
                meta_tags = soup.find_all('meta')
                for meta in meta_tags:
                    content = meta.get('content', '').lower()
                    if any(ai_term in content for ai_term in ['ai', 'artificial intelligence', 'machine learning', 'automation']):
                        ai_tech_indicators.append(f"AI Meta Tag: {meta.get('name', 'unknown')}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„ JavaScript
                scripts = soup.find_all('script')
                for script in scripts:
                    script_content = script.string or ''
                    if any(ai_term in script_content.lower() for ai_term in ['ai', 'artificial intelligence', 'machine learning', 'automation']):
                        ai_tech_indicators.append("AI JavaScript Integration")
                        break
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„å…§å®¹
                page_text = soup.get_text().lower()
                ai_content_signals = []
                
                ai_keywords = [
                    'artificial intelligence', 'machine learning', 'ai', 'automation',
                    'predictive analytics', 'natural language processing', 'nlp',
                    'computer vision', 'deep learning', 'neural networks'
                ]
                
                for keyword in ai_keywords:
                    if keyword in page_text:
                        ai_content_signals.append(keyword)
                
                ai_leader_analysis["ai_technology_indicators"] = ai_tech_indicators
                ai_leader_analysis["ai_content_signals"] = ai_content_signals
                
                # è¨ˆç®— AI é ˜å°è€…åˆ†æ•¸
                score = 0
                score += len(ai_tech_indicators) * 10
                score += len(ai_content_signals) * 5
                
                # æ¨¡æ“¬ AI åƒèˆ‡åº¦æŒ‡æ¨™
                ai_leader_analysis["ai_engagement_metrics"] = {
                    "ai_mentions_count": len(ai_content_signals),
                    "ai_technology_integration": len(ai_tech_indicators),
                    "ai_content_frequency": len(ai_content_signals) / max(len(page_text.split()), 1) * 1000
                }
                
                ai_leader_analysis["ai_leader_score"] = min(score, 100)
                
                # åˆ¤æ–· AI é ˜å°åœ°ä½
                if score >= 80:
                    ai_leader_analysis["ai_leadership_position"] = "leader"
                    st.success("ğŸ† è­˜åˆ¥ç‚º AI é ˜å°è€…")
                elif score >= 50:
                    ai_leader_analysis["ai_leadership_position"] = "emerging"
                    st.info("ğŸ“ˆ AI æ–°èˆˆé ˜å°è€…")
                elif score >= 20:
                    ai_leader_analysis["ai_leadership_position"] = "follower"
                    st.warning("ğŸ“Š AI è·Ÿéš¨è€…")
                else:
                    ai_leader_analysis["ai_leadership_position"] = "laggard"
                    st.error("âš ï¸ AI è½å¾Œè€…")
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æ AI é ˜å°è€…æŒ‡æ¨™: {str(e)}")
        
        return ai_leader_analysis
    
    def _analyze_dynamic_media_weights(self, target_website: str) -> dict:
        """
        åˆ†æåª’é«”æ¬Šé‡èˆ‡è¦†è“‹ç‡ï¼š
        - æ–°èé¡ä¾†æºç”¨ Google News API æŸ¥è©¢å“ç‰Œ/ç¶²ç«™æ˜¯å¦è¢«æåŠã€‚
        - å…¶ä»–é¡å‹ï¼ˆç¤¾ç¾¤ã€è«–å£‡ã€å½±éŸ³ã€Wikiï¼‰ç”¨ Gemini LLM æ¨è«–æ˜¯å¦å¸¸è¢«æåŠã€‚
        - ç¶œåˆä¿¡ä»»åº¦ã€æåŠæƒ…æ³ã€ä¾†æºå¤šæ¨£æ€§è¨ˆç®—åˆ†æ•¸ã€‚
        """
        media_dict = self.market_media_dict
        brand = self.brand
        product_category = self.product_category
        result = {"æ–°è": [], "ç¤¾ç¾¤": [], "è«–å£‡": [], "å½±éŸ³": [], "Wiki": []}
        total_weight = 0
        covered_weight = 0
        covered_count = 0
        total_count = 0
        # 1. æ–°èé¡ï¼šGoogle News API
        api_key = "bce856a8587d46ff84050efba536c445"
        endpoint = "https://newsapi.org/v2/everything"
        for media in media_dict.get("æ–°è", []):
            name = media.get("name")
            trust_score = media.get("trust_score", 80)
            llm_favorite = media.get("llm_favorite", False)
            params = {
                "q": f"{brand} OR {product_category}",
                "sources": "",
                "apiKey": api_key,
                "language": "zh",
                "sortBy": "publishedAt",
                "pageSize": 3
            }
            found = False
            try:
                resp = requests.get(endpoint, params=params, timeout=10)
                if resp.status_code == 200:
                    data = resp.json()
                    for article in data.get("articles", []):
                        if name in article.get("source", {}).get("name", ""):
                            found = True
                            break
            except Exception:
                pass
            result["æ–°è"].append({
                "name": name,
                "llm_favorite": llm_favorite,
                "trust_score": trust_score,
                "reason": media.get("reason", ""),
                "covered": found,
                "source_type": "æ–°è"
            })
            total_weight += trust_score
            total_count += 1
            if found:
                covered_weight += trust_score
                covered_count += 1
        # 2. å…¶ä»–é¡å‹ï¼šLLM è¼”åŠ©æ¨è«–
        if self.gemini_model:
            for media_type in ["ç¤¾ç¾¤", "è«–å£‡", "å½±éŸ³", "Wiki"]:
                for media in media_dict.get(media_type, []):
                    name = media.get("name")
                    trust_score = media.get("trust_score", 80)
                    llm_favorite = media.get("llm_favorite", False)
                    prompt = f"""
è«‹æ ¹æ“šä¸‹åˆ—è³‡è¨Šï¼Œæ¨è«–å“ç‰Œ/ç¶²ç«™æ˜¯å¦å¸¸è¢«æ­¤ä¾†æºæåŠã€å¼•ç”¨ã€åµŒå…¥æˆ–é€£çµï¼š
- å“ç‰Œï¼š{brand}
- å“é¡ï¼š{product_category}
- ä¾†æºåç¨±ï¼š{name}
- å®˜ç¶²ï¼š{target_website}
è«‹å›å‚³ï¼š
{{
  "covered": true/false, // æ˜¯å¦å¸¸è¢«æåŠ
  "score": 0-100, // æ¨è«–åˆ†æ•¸ï¼Œæ„ˆé«˜æ„ˆå¸¸è¢«æåŠ
  "reason": "æ¨è«–ä¾æ“š"
}}
"""
                    try:
                        response = self.gemini_model.generate_content(prompt)
                        text = response.text.strip()
                        if text.startswith('```json'):
                            text = text[7:]
                        if text.endswith('```'):
                            text = text[:-3]
                        text = text.strip()
                        info = json.loads(text)
                        covered = info.get("covered", False)
                        score = info.get("score", 0)
                        reason = info.get("reason", "")
                    except Exception:
                        covered = False
                        score = 0
                        reason = "LLM å›æ‡‰å¤±æ•—"
                    result[media_type].append({
                        "name": name,
                        "llm_favorite": llm_favorite,
                        "trust_score": trust_score,
                        "reason": media.get("reason", ""),
                        "covered": covered,
                        "llm_score": score,
                        "llm_reason": reason,
                        "source_type": media_type
                    })
                    total_weight += trust_score
                    total_count += 1
                    if covered:
                        covered_weight += trust_score
                        covered_count += 1
        # 3. åˆ†æ•¸è¨ˆç®—
        coverage_rate = covered_count / total_count if total_count else 0
        weighted_coverage = covered_weight / total_weight if total_weight else 0
        media_weight_score = int(weighted_coverage * 100)
        return {
            "media_coverage_score": media_weight_score,
            "coverage_rate": coverage_rate,
            "covered_count": covered_count,
            "total_count": total_count,
            "sources": result
        }
    
    def _analyze_competitor_benchmarks(self, target_website: str, competitors: List[str] = []) -> Dict:
        """åˆ†æç«¶çˆ­å°æ‰‹åŸºæº–"""
        st.write("ğŸ† åˆ†æç«¶çˆ­å°æ‰‹åŸºæº–...")
        
        if not competitors:
            # æ¨¡æ“¬ç«¶çˆ­å°æ‰‹
            competitors = [
                "competitor1.com",
                "competitor2.com",
                "competitor3.com"
            ]
        
        competitor_benchmarking = {
            "competitor_analysis": [],
            "market_position": "unknown",
            "competitive_advantages": [],
            "improvement_opportunities": []
        }
        
        try:
            # åˆ†æç›®æ¨™ç¶²ç«™
            target_analysis = self._analyze_single_competitor(target_website, "Target")
            
            # åˆ†æç«¶çˆ­å°æ‰‹
            competitor_analyses = []
            for competitor in competitors:
                try:
                    comp_analysis = self._analyze_single_competitor(competitor, f"Competitor {competitors.index(competitor) + 1}")
                    competitor_analyses.append(comp_analysis)
                except Exception as e:
                    st.warning(f"âš ï¸ ç„¡æ³•åˆ†æç«¶çˆ­å°æ‰‹ {competitor}: {str(e)}")
            
            competitor_benchmarking["competitor_analysis"] = [target_analysis] + competitor_analyses
            
            # è¨ˆç®—å¸‚å ´åœ°ä½
            target_score = target_analysis["overall_score"]
            competitor_scores = [comp["overall_score"] for comp in competitor_analyses if comp["overall_score"] > 0]
            
            if competitor_scores:
                avg_competitor_score = sum(competitor_scores) / len(competitor_scores)
                if target_score > avg_competitor_score * 1.2:
                    competitor_benchmarking["market_position"] = "leader"
                    st.success("ğŸ† å¸‚å ´é ˜å°è€…")
                elif target_score > avg_competitor_score:
                    competitor_benchmarking["market_position"] = "strong"
                    st.info("ğŸ’ª å¼·å‹¢ç«¶çˆ­è€…")
                elif target_score > avg_competitor_score * 0.8:
                    competitor_benchmarking["market_position"] = "average"
                    st.warning("ğŸ“Š å¹³å‡æ°´æº–")
                else:
                    competitor_benchmarking["market_position"] = "laggard"
                    st.error("âš ï¸ è½å¾Œç«¶çˆ­è€…")
            
            # è­˜åˆ¥ç«¶çˆ­å„ªå‹¢
            if target_analysis["ai_leader_score"] > 50:
                competitor_benchmarking["competitive_advantages"].append("AI Leadership")
            
            if target_analysis["social_authority_score"] > 60:
                competitor_benchmarking["competitive_advantages"].append("Strong Social Presence")
            
            if target_analysis["media_coverage_score"] > 70:
                competitor_benchmarking["competitive_advantages"].append("Positive Media Coverage")
            
            # è­˜åˆ¥æ”¹å–„æ©Ÿæœƒ
            if target_analysis["ai_leader_score"] < 30:
                competitor_benchmarking["improvement_opportunities"].append("Enhance AI Technology Integration")
            
            if target_analysis["social_authority_score"] < 40:
                competitor_benchmarking["improvement_opportunities"].append("Strengthen Social Media Presence")
            
            if target_analysis["media_coverage_score"] < 50:
                competitor_benchmarking["improvement_opportunities"].append("Improve Media Relations")
            
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•å®Œæˆç«¶çˆ­å°æ‰‹åŸºæº–åˆ†æ: {str(e)}")
        
        return competitor_benchmarking
    
    def _analyze_single_competitor(self, website: str, name: str) -> Dict:
        """åˆ†æå–®ä¸€ç«¶çˆ­å°æ‰‹"""
        if not website.startswith(('http://', 'https://')):
            website = 'https://' + website
        
        try:
            response = self.session.get(website, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ¨¡æ“¬åˆ†æçµæœ
                analysis = {
                    "name": name,
                    "website": website,
                    "ai_leader_score": random.randint(20, 90),
                    "social_authority_score": random.randint(30, 85),
                    "media_coverage_score": random.randint(40, 95),
                    "content_quality_score": random.randint(50, 95),
                    "overall_score": 0
                }
                
                # è¨ˆç®—ç¸½åˆ†
                analysis["overall_score"] = (
                    analysis["ai_leader_score"] * 0.3 +
                    analysis["social_authority_score"] * 0.25 +
                    analysis["media_coverage_score"] * 0.25 +
                    analysis["content_quality_score"] * 0.2
                )
                
                return analysis
            else:
                return {
                    "name": name,
                    "website": website,
                    "ai_leader_score": 0,
                    "social_authority_score": 0,
                    "media_coverage_score": 0,
                    "content_quality_score": 0,
                    "overall_score": 0
                }
        except Exception as e:
            return {
                "name": name,
                "website": website,
                "ai_leader_score": 0,
                "social_authority_score": 0,
                "media_coverage_score": 0,
                "content_quality_score": 0,
                "overall_score": 0,
                "error": str(e)
            }
    
    def _analyze_trends_and_predictions(self, target_website: str) -> Dict:
        """åˆ†æè¶¨å‹¢èˆ‡é æ¸¬"""
        st.write("ğŸ“ˆ åˆ†æè¶¨å‹¢èˆ‡é æ¸¬...")
        
        trend_analysis = {
            "current_trends": [],
            "predicted_growth": {},
            "market_opportunities": [],
            "risk_factors": []
        }
        
        try:
            # æ¨¡æ“¬ç•¶å‰è¶¨å‹¢
            current_trends = [
                "AI Integration Acceleration",
                "Voice Search Optimization",
                "Video Content Dominance",
                "Personalization at Scale",
                "Sustainability Focus"
            ]
            
            trend_analysis["current_trends"] = current_trends
            
            # æ¨¡æ“¬é æ¸¬æˆé•·
            trend_analysis["predicted_growth"] = {
                "ai_adoption_rate": random.uniform(15, 35),
                "content_consumption_growth": random.uniform(20, 40),
                "social_engagement_increase": random.uniform(10, 25),
                "market_share_growth": random.uniform(5, 15)
            }
            
            # è­˜åˆ¥å¸‚å ´æ©Ÿæœƒ
            market_opportunities = [
                "AI-Powered Content Personalization",
                "Voice-First Content Strategy",
                "Interactive Video Experiences",
                "Sustainability Messaging",
                "Micro-Moment Optimization"
            ]
            
            trend_analysis["market_opportunities"] = market_opportunities
            
            # è­˜åˆ¥é¢¨éšªå› ç´ 
            risk_factors = [
                "AI Regulation Changes",
                "Privacy Policy Updates",
                "Algorithm Changes",
                "Competitive Pressure",
                "Technology Disruption"
            ]
            
            trend_analysis["risk_factors"] = risk_factors
            
            st.success("âœ… è¶¨å‹¢åˆ†æå®Œæˆ")
            
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æè¶¨å‹¢: {str(e)}")
        
        return trend_analysis
    
    def _generate_strategic_recommendations(self, ai_leader_analysis: Dict, dynamic_media_weights: Dict, 
                                         competitor_benchmarking: Dict, trend_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆç­–ç•¥å»ºè­°"""
        if not self.gemini_model:
            return self._generate_fallback_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
        
        st.write("ğŸ¯ ç”Ÿæˆç­–ç•¥å»ºè­°...")
        
        try:
            analysis_data = {
                "ai_leader_analysis": ai_leader_analysis,
                "dynamic_media_weights": dynamic_media_weights,
                "competitor_benchmarking": competitor_benchmarking,
                "trend_analysis": trend_analysis
            }
            
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ SIE ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­åˆ¶å®š E-E-A-T ç«¶çˆ­ç­–ç•¥ã€‚

è«‹æ ¹æ“šä»¥ä¸‹åˆ†æçµæœï¼Œæä¾›å…·é«”ã€å¯åŸ·è¡Œçš„ç­–ç•¥å»ºè­°ï¼š

åˆ†ææ•¸æ“šï¼š
{json.dumps(analysis_data, indent=2, ensure_ascii=False)}

è«‹ä»¥ JSON æ ¼å¼å›å‚³ç­–ç•¥å»ºè­°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "strategic_recommendations": [
    {{
      "strategy": "ç­–ç•¥åç¨±",
      "description": "ç­–ç•¥æè¿°",
      "priority": "High/Medium/Low",
      "timeline": "Short-term/Medium-term/Long-term",
      "expected_impact": "é æœŸå½±éŸ¿",
      "implementation_steps": ["æ­¥é©Ÿ1", "æ­¥é©Ÿ2", "æ­¥é©Ÿ3"]
    }}
  ]
}}

è«‹ç¢ºä¿å»ºè­°å…·é«”ã€å¯åŸ·è¡Œï¼Œä¸¦é‡å°ç«¶çˆ­å„ªå‹¢æå‡ã€‚
"""
            
            response = self.gemini_model.generate_content(prompt)
            recommendations_data = json.loads(response.text)
            return recommendations_data.get("strategic_recommendations", [])
            
        except Exception as e:
            st.warning(f"âš ï¸ Gemini API ç”Ÿæˆç­–ç•¥å»ºè­°å¤±æ•—: {str(e)}")
            return self._generate_fallback_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
    
    def _generate_fallback_strategic_recommendations(self, ai_leader_analysis: Dict, dynamic_media_weights: Dict, 
                                                   competitor_benchmarking: Dict, trend_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚™ç”¨ç­–ç•¥å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ AI é ˜å°è€…åˆ†æçš„å»ºè­°
        if ai_leader_analysis["ai_leader_score"] < 50:
            recommendations.append({
                "strategy": "AI Technology Integration",
                "description": "åŠ å¼· AI æŠ€è¡“æ•´åˆï¼Œæå‡ AI é ˜å°è€…åœ°ä½",
                "priority": "High",
                "timeline": "Medium-term",
                "expected_impact": "æå‡ AI é ˜å°è€…åˆ†æ•¸ 30-50%",
                "implementation_steps": [
                    "è©•ä¼°ç¾æœ‰ AI æŠ€è¡“åŸºç¤",
                    "åˆ¶å®š AI æ•´åˆè·¯ç·šåœ–",
                    "å¯¦æ–½ AI å…§å®¹ç”Ÿæˆå·¥å…·",
                    "å»ºç«‹ AI æ•ˆèƒ½ç›£æ§ç³»çµ±"
                ]
            })
        
        # åŸºæ–¼ç¤¾äº¤åª’é«”åˆ†æçš„å»ºè­°
        if dynamic_media_weights["social_media_presence"]["social_authority_score"] < 60:
            recommendations.append({
                "strategy": "Social Media Authority Building",
                "description": "å»ºç«‹å¼·å¤§çš„ç¤¾äº¤åª’é«”æ¬Šå¨ï¼Œæå‡å“ç‰Œå½±éŸ¿åŠ›",
                "priority": "Medium",
                "timeline": "Long-term",
                "expected_impact": "æå‡ç¤¾äº¤åª’é«”æ¬Šå¨åˆ†æ•¸ 40-60%",
                "implementation_steps": [
                    "åˆ¶å®šç¤¾äº¤åª’é«”å…§å®¹ç­–ç•¥",
                    "å»ºç«‹å½±éŸ¿è€…åˆä½œé—œä¿‚",
                    "å¯¦æ–½ç¤¾ç¾¤ç®¡ç†å·¥å…·",
                    "å®šæœŸç™¼å¸ƒé«˜åƒ¹å€¼å…§å®¹"
                ]
            })
        
        # åŸºæ–¼ç«¶çˆ­åˆ†æçš„å»ºè­°
        if competitor_benchmarking["market_position"] in ["average", "laggard"]:
            recommendations.append({
                "strategy": "Competitive Differentiation",
                "description": "å»ºç«‹ç¨ç‰¹çš„ç«¶çˆ­å„ªå‹¢ï¼Œè¶…è¶Šç«¶çˆ­å°æ‰‹",
                "priority": "High",
                "timeline": "Medium-term",
                "expected_impact": "æå‡å¸‚å ´åœ°ä½è‡³å¼·å‹¢ç«¶çˆ­è€…",
                "implementation_steps": [
                    "è­˜åˆ¥ç¨ç‰¹åƒ¹å€¼ä¸»å¼µ",
                    "é–‹ç™¼å·®ç•°åŒ–å…§å®¹ç­–ç•¥",
                    "å»ºç«‹å“ç‰Œæ¬Šå¨",
                    "å¯¦æ–½å‰µæ–°æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ"
                ]
            })
        
        return recommendations

    def _generate_market_media_with_llm(self, market: str, product_category: str, brand: str) -> dict:
        """
        ç”¨ LLM ç”¢ç”Ÿä¸»æµåª’é«”/è«–å£‡/ç¤¾ç¾¤/å½±éŸ³/Wikiåå–®ï¼Œä¸¦æ¨™è¨» llm_favoriteã€ä¿¡ä»»åº¦åˆ†æ•¸ã€æ’åºä¾æ“šã€‚
        """
        if self.gemini_model:
            prompt = f"""
è«‹æ ¹æ“šä¸‹åˆ—è³‡è¨Šï¼Œåˆ—å‡º{market}å¸‚å ´ã€{product_category}å“é¡ã€{brand}å“ç‰Œæœ€å…·ä»£è¡¨æ€§çš„ä¾†æºï¼Œåˆ†ç‚ºï¼š
- æ–°èï¼ˆè¡Œæ¥­æ–°èã€ç¶œåˆæ–°èï¼‰
- ç¤¾ç¾¤ï¼ˆFacebookã€Instagramã€Twitterã€LinkedInç­‰ï¼‰
- è«–å£‡ï¼ˆPTTã€Dcardã€Redditç­‰ï¼‰
- å½±éŸ³ï¼ˆYouTubeã€Bilibiliç­‰ï¼‰
- Wikiï¼ˆWikipediaç­‰ï¼‰
æ¯é¡è«‹åˆ—å‡º3-5å€‹ä¾†æºï¼Œä¸¦é‡å°æ¯å€‹ä¾†æºå›å‚³ï¼š
- name: ä¾†æºåç¨±
- llm_favorite: æ˜¯å¦ç‚º LLM æœ€å¸¸å¼•ç”¨
- trust_score: LLM å°è©²ä¾†æºçš„ä¿¡ä»»åº¦åˆ†æ•¸ï¼ˆ0-100ï¼‰
- reason: æ’åºä¾æ“šï¼ˆå¦‚æµé‡ã€äº’å‹•æ•¸ã€å¼•ç”¨æ¬¡æ•¸ã€æœå°‹æ’åç­‰ï¼‰
è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
{
  "æ–°è": [{{"name": "åª’é«”åç¨±", "llm_favorite": true/false, "trust_score": 0-100, "reason": "æ’åºä¾æ“š"}}, ...],
  "ç¤¾ç¾¤": [{{"name": "ç¤¾ç¾¤åç¨±", "llm_favorite": true/false, "trust_score": 0-100, "reason": "æ’åºä¾æ“š"}}, ...],
  "è«–å£‡": [{{"name": "è«–å£‡åç¨±", "llm_favorite": true/false, "trust_score": 0-100, "reason": "æ’åºä¾æ“š"}}, ...],
  "å½±éŸ³": [{{"name": "å½±éŸ³å¹³å°åç¨±", "llm_favorite": true/false, "trust_score": 0-100, "reason": "æ’åºä¾æ“š"}}, ...],
  "Wiki": [{{"name": "Wikiåç¨±", "llm_favorite": true/false, "trust_score": 0-100, "reason": "æ’åºä¾æ“š"}}, ...]
}
"""
            try:
                response = self.gemini_model.generate_content(prompt)
                text = response.text.strip()
                if text.startswith('```json'):
                    text = text[7:]
                if text.endswith('```'):
                    text = text[:-3]
                text = text.strip()
                media_dict = json.loads(text)
                return media_dict
            except Exception:
                pass
        # fallback éœæ…‹ç¯„ä¾‹
        return {
            "æ–°è": [
                {"name": "ç¶“æ¿Ÿæ—¥å ±", "llm_favorite": True, "trust_score": 95, "reason": "è¡Œæ¥­å½±éŸ¿åŠ›é«˜"},
                {"name": "å·¥å•†æ™‚å ±", "llm_favorite": False, "trust_score": 88, "reason": "è²¡ç¶“æ–°èå¼•ç”¨å¤š"}
            ],
            "è«–å£‡": [
                {"name": "Mobile01", "llm_favorite": False, "trust_score": 80, "reason": "ç§‘æŠ€è¨è«–ç†±åº¦é«˜"},
                {"name": "PTT Tech_Job", "llm_favorite": True, "trust_score": 90, "reason": "å·¥ç¨‹å¸«ç¤¾ç¾¤æ´»èº"}
            ],
            "ç¤¾ç¾¤": [
                {"name": "Dcard", "llm_favorite": False, "trust_score": 85, "reason": "å¹´è¼•æ—ç¾¤æ´»èº"},
                {"name": "LinkedIn", "llm_favorite": True, "trust_score": 92, "reason": "å°ˆæ¥­äººå£«èšé›†"}
            ],
            "å½±éŸ³": [
                {"name": "YouTube", "llm_favorite": True, "trust_score": 98, "reason": "å½±éŸ³æµé‡æœ€å¤§"},
                {"name": "Bilibili", "llm_favorite": False, "trust_score": 80, "reason": "å¹´è¼•ç”¨æˆ¶å¤š"}
            ],
            "Wiki": [
                {"name": "Wikipedia", "llm_favorite": True, "trust_score": 99, "reason": "å…¨çƒæœ€å¤§ç™¾ç§‘"}
            ]
        }

    def _fetch_real_media_mentions(self, market_media_dict: dict) -> dict:
        """
        ä½¿ç”¨ Google News API æŸ¥è©¢ LLM ç”¢ç”Ÿçš„åª’é«”/è«–å£‡/ç¤¾ç¾¤åå–®ï¼Œå–å¾—æ¨™é¡Œã€é€£çµã€æ—¥æœŸã€æ‘˜è¦ã€ä¾†æºã€llm_favoriteã€‚
        """
        api_key = "bce856a8587d46ff84050efba536c445"
        endpoint = "https://newsapi.org/v2/everything"
        result = {"æ–°è": [], "è«–å£‡": [], "ç¤¾ç¾¤": []}
        for media_type in ["æ–°è", "è«–å£‡", "ç¤¾ç¾¤"]:
            for media in market_media_dict.get(media_type, []):
                name = media.get("name")
                llm_favorite = media.get("llm_favorite", False)
                params = {
                    "q": name,
                    "apiKey": api_key,
                    "language": "zh",
                    "sortBy": "publishedAt",
                    "pageSize": 3
                }
                try:
                    resp = requests.get(endpoint, params=params, timeout=10)
                    if resp.status_code == 200:
                        data = resp.json()
                        for article in data.get("articles", []):
                            result[media_type].append({
                                "media": name,
                                "llm_favorite": llm_favorite,
                                "title": article.get("title"),
                                "url": article.get("url"),
                                "date": article.get("publishedAt"),
                                "summary": article.get("description"),
                                "source": article.get("source", {}).get("name")
                            })
                except Exception:
                    continue
        return result

    def _llm_recommend_leaders(self, market: str, product_category: str, brand: str, official_site: str) -> list:
        """
        ç”¨ LLM ç”¢ç”Ÿè¡Œæ¥­/å¸‚å ´/ç”¢å“é ˜å°è€…åå–®ï¼Œå«å“ç‰Œ/å…¬å¸/ç”¢å“/å®˜ç¶²/æ¨è–¦èªªæ˜/æ˜¯å¦æ¨™ç«¿ã€‚
        """
        if not self.gemini_model:
            # fallback ç¯„ä¾‹
            return [
                {"name": "å°ç©é›»", "website": "https://www.tsmc.com/", "reason": "å…¨çƒåŠå°é«”è£½é€ é ˜å°è€…ï¼ŒæŠ€è¡“å‰µæ–°èˆ‡å¸‚ä½”ç‡æœ€é«˜ã€‚", "is_benchmark": True},
                {"name": "è¯é›»", "website": "https://www.umc.com/", "reason": "å°ç£ç¬¬äºŒå¤§æ™¶åœ“ä»£å·¥å» ï¼Œå…·åœ‹éš›ç«¶çˆ­åŠ›ã€‚", "is_benchmark": False}
            ]
        prompt = f"""
è«‹æ ¹æ“šä¸‹åˆ—è³‡è¨Šï¼Œåˆ—å‡º{market}å¸‚å ´ã€{product_category}å“é¡ã€{brand}å“ç‰Œé ˜åŸŸæœ€å…·æ¬Šå¨èˆ‡ç«¶çˆ­åŠ›çš„å“ç‰Œ/å…¬å¸/ç”¢å“ï¼ˆå«æœ¬åœ°èˆ‡åœ‹éš›ï¼‰ï¼Œæ¯å€‹è«‹é™„ä¸Šç°¡è¦èªªæ˜èˆ‡å®˜ç¶²é€£çµï¼Œä¸¦æ¨™è¨»æ˜¯å¦ç‚ºè¡Œæ¥­æ¨™ç«¿ï¼š
å“ç‰Œ/å®˜ç¶²ï¼š{brand}ï¼ˆ{official_site}ï¼‰
è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
[
  {{"name": "å“ç‰Œ/å…¬å¸/ç”¢å“åç¨±", "website": "å®˜ç¶²é€£çµ", "reason": "æ¨è–¦åŸå› ", "is_benchmark": true/false}},
  ...
]
"""
        try:
            response = self.gemini_model.generate_content(prompt)
            text = response.text.strip()
            if text.startswith('```json'):
                text = text[7:]
            if text.endswith('```'):
                text = text[:-3]
            text = text.strip()
            leaders = json.loads(text)
            return leaders
        except Exception:
            return []

    def _llm_compare_with_benchmarks(self, brand: str, official_site: str, market: str, product_category: str, leaders: list) -> dict:
        """
        ç”¨ LLM æ¯”å°æœ¬å“ç‰Œèˆ‡è¡Œæ¥­æ¨™ç«¿ï¼Œç”¢ç”Ÿå„ªåŠ£å‹¢/å·®è·åˆ†æ•¸/å»ºè­°ã€‚
        """
        if not self.gemini_model or not leaders:
            return {"summary": "ç„¡æ³•å–å¾— LLM æ¨™ç«¿æ¯”å°åˆ†æï¼ˆç¼ºå°‘ LLM æˆ–æ¨™ç«¿è³‡æ–™ï¼‰"}
        prompt = f"""
è«‹æ ¹æ“šä¸‹åˆ—è³‡è¨Šï¼Œåˆ†ææœ¬å“ç‰Œèˆ‡è¡Œæ¥­æ¨™ç«¿çš„å…·é«”å·®ç•°ï¼Œåˆ—å‡ºå„ªå‹¢ã€åŠ£å‹¢ã€å·®è·åˆ†æ•¸èˆ‡å…·é«”å»ºè­°ï¼š
- è¡Œæ¥­/å¸‚å ´ï¼š{market}
- ç”¢å“/å“é¡ï¼š{product_category}
- æœ¬å“ç‰Œï¼š{brand}ï¼ˆ{official_site}ï¼‰
- è¡Œæ¥­æ¨™ç«¿ï¼š{json.dumps(leaders, ensure_ascii=False)}
è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
{{
  "gap_score": 0~100,  // æœ¬å“ç‰Œèˆ‡æ¨™ç«¿çš„æ•´é«”å·®è·åˆ†æ•¸ï¼Œåˆ†æ•¸è¶Šé«˜å·®è·è¶Šå¤§
  "advantages": ["å„ªå‹¢1", "å„ªå‹¢2", ...],
  "disadvantages": ["åŠ£å‹¢1", "åŠ£å‹¢2", ...],
  "recommendations": ["å»ºè­°1", "å»ºè­°2", ...],
  "summary": "ç°¡è¦ç¸½çµ"
}}
"""
        try:
            response = self.gemini_model.generate_content(prompt)
            text = response.text.strip()
            if text.startswith('```json'):
                text = text[7:]
            if text.endswith('```'):
                text = text[:-3]
            text = text.strip()
            gap = json.loads(text)
            return gap
        except Exception:
            return {"summary": "LLM æ¯”å°å¤±æ•—æˆ–æ ¼å¼éŒ¯èª¤"}

def run_eeat_benchmarking(target_website: str, competitors: List[str] = [], gemini_api_key: Optional[str] = None, industry: Optional[str] = None, product_category: Optional[str] = None, brand: Optional[str] = None) -> Dict:
    """åŸ·è¡Œ E-E-A-T åŸºæº–åˆ†æçš„ä¸»å‡½å¼ï¼ˆæ”¯æ´è¡Œæ¥­/å“é¡/å“ç‰Œï¼‰"""
    analyzer = EEATBenchmarkingAnalyzer(gemini_api_key, industry, product_category, brand)
    return analyzer.analyze_eeat_benchmarking(target_website, competitors) 