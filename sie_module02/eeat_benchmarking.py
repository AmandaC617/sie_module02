import requests
import json
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os
from typing import Dict, List, Optional, Tuple
import streamlit as st
from datetime import datetime, timedelta
import random

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: google-generativeai æœªå®‰è£ï¼ŒGemini API åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

class EEATBenchmarkingAnalyzer:
    """å‹•æ…‹ E-E-A-T è©•ä¼°èˆ‡ç«¶çˆ­åŸºæº–åˆ†æå™¨"""
    
    def __init__(self, gemini_api_key: Optional[str] = None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SIE-Benchmarking-Tool/1.0 (contact@example.com)'
        })
        
        # åˆå§‹åŒ– Gemini API
        if gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        else:
            self.gemini_model = None
    
    def analyze_eeat_benchmarking(self, target_website: str, competitors: List[str] = []) -> Dict:
        """åŸ·è¡Œå‹•æ…‹ E-E-A-T è©•ä¼°èˆ‡ç«¶çˆ­åŸºæº–åˆ†æ"""
        if not target_website.startswith(('http://', 'https://')):
            target_website = 'https://' + target_website
        
        st.info(f"ğŸ” æ­£åœ¨åˆ†æç›®æ¨™ç¶²ç«™: {target_website}")
        
        try:
            # 1. AI é ˜å°è€…è­˜åˆ¥èˆ‡åˆ†æ
            ai_leader_analysis = self._identify_ai_leaders(target_website)
            
            # 2. å‹•æ…‹åª’é«”æ¬Šé‡è©•ä¼°
            dynamic_media_weights = self._analyze_dynamic_media_weights(target_website)
            
            # 3. ç«¶çˆ­å°æ‰‹åŸºæº–åˆ†æ
            competitor_benchmarking = self._analyze_competitor_benchmarks(target_website, competitors)
            
            # 4. è¶¨å‹¢è¿½è¹¤èˆ‡é æ¸¬
            trend_analysis = self._analyze_trends_and_predictions(target_website)
            
            # 5. ç”Ÿæˆç­–ç•¥å»ºè­°
            strategic_recommendations = self._generate_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
            
            return {
                "eeat_benchmarking": {
                    "ai_leader_analysis": ai_leader_analysis,
                    "dynamic_media_weights": dynamic_media_weights,
                    "competitor_benchmarking": competitor_benchmarking,
                    "trend_analysis": trend_analysis,
                    "strategic_recommendations": strategic_recommendations
                }
            }
            
        except Exception as e:
            st.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {"error": str(e)}
    
    def _identify_ai_leaders(self, target_website: str) -> Dict:
        """è­˜åˆ¥èˆ‡åˆ†æ AI é ˜å°è€…"""
        st.write("ğŸ¤– è­˜åˆ¥ AI é ˜å°è€…...")
        
        ai_leader_analysis = {
            "ai_leader_score": 0,
            "ai_technology_indicators": [],
            "ai_content_signals": [],
            "ai_engagement_metrics": {},
            "ai_leadership_position": "unknown"
        }
        
        try:
            response = self.session.get(target_website, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥ AI æŠ€è¡“æŒ‡æ¨™
                ai_tech_indicators = []
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„ meta æ¨™ç±¤
                meta_tags = soup.find_all('meta')
                for meta in meta_tags:
                    content = meta.get('content', '').lower()
                    if any(ai_term in content for ai_term in ['ai', 'artificial intelligence', 'machine learning', 'automation']):
                        ai_tech_indicators.append(f"AI Meta Tag: {meta.get('name', 'unknown')}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„ JavaScript
                scripts = soup.find_all('script')
                for script in scripts:
                    script_content = script.string or ''
                    if any(ai_term in script_content.lower() for ai_term in ['ai', 'artificial intelligence', 'machine learning', 'automation']):
                        ai_tech_indicators.append("AI JavaScript Integration")
                        break
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ AI ç›¸é—œçš„å…§å®¹
                page_text = soup.get_text().lower()
                ai_content_signals = []
                
                ai_keywords = [
                    'artificial intelligence', 'machine learning', 'ai', 'automation',
                    'predictive analytics', 'natural language processing', 'nlp',
                    'computer vision', 'deep learning', 'neural networks'
                ]
                
                for keyword in ai_keywords:
                    if keyword in page_text:
                        ai_content_signals.append(keyword)
                
                ai_leader_analysis["ai_technology_indicators"] = ai_tech_indicators
                ai_leader_analysis["ai_content_signals"] = ai_content_signals
                
                # è¨ˆç®— AI é ˜å°è€…åˆ†æ•¸
                score = 0
                score += len(ai_tech_indicators) * 10
                score += len(ai_content_signals) * 5
                
                # æ¨¡æ“¬ AI åƒèˆ‡åº¦æŒ‡æ¨™
                ai_leader_analysis["ai_engagement_metrics"] = {
                    "ai_mentions_count": len(ai_content_signals),
                    "ai_technology_integration": len(ai_tech_indicators),
                    "ai_content_frequency": len(ai_content_signals) / max(len(page_text.split()), 1) * 1000
                }
                
                ai_leader_analysis["ai_leader_score"] = min(score, 100)
                
                # åˆ¤æ–· AI é ˜å°åœ°ä½
                if score >= 80:
                    ai_leader_analysis["ai_leadership_position"] = "leader"
                    st.success("ğŸ† è­˜åˆ¥ç‚º AI é ˜å°è€…")
                elif score >= 50:
                    ai_leader_analysis["ai_leadership_position"] = "emerging"
                    st.info("ğŸ“ˆ AI æ–°èˆˆé ˜å°è€…")
                elif score >= 20:
                    ai_leader_analysis["ai_leadership_position"] = "follower"
                    st.warning("ğŸ“Š AI è·Ÿéš¨è€…")
                else:
                    ai_leader_analysis["ai_leadership_position"] = "laggard"
                    st.error("âš ï¸ AI è½å¾Œè€…")
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æ AI é ˜å°è€…æŒ‡æ¨™: {str(e)}")
        
        return ai_leader_analysis
    
    def _analyze_dynamic_media_weights(self, target_website: str) -> Dict:
        """åˆ†æå‹•æ…‹åª’é«”æ¬Šé‡"""
        st.write("ğŸ“Š åˆ†æå‹•æ…‹åª’é«”æ¬Šé‡...")
        
        dynamic_media_weights = {
            "media_mentions": {
                "recent_mentions": [],
                "mention_sentiment": "neutral",
                "media_coverage_score": 0
            },
            "social_media_presence": {
                "platforms": [],
                "engagement_metrics": {},
                "social_authority_score": 0
            },
            "content_distribution": {
                "content_types": [],
                "distribution_channels": [],
                "reach_metrics": {}
            }
        }
        
        try:
            response = self.session.get(target_website, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æª¢æŸ¥ç¤¾äº¤åª’é«”é€£çµ
                social_platforms = []
                social_links = soup.find_all('a', href=True)
                
                social_platform_patterns = {
                    'linkedin': 'linkedin.com',
                    'twitter': 'twitter.com',
                    'facebook': 'facebook.com',
                    'instagram': 'instagram.com',
                    'youtube': 'youtube.com'
                }
                
                for link in social_links:
                    href = link['href'].lower()
                    for platform, pattern in social_platform_patterns.items():
                        if pattern in href and platform not in social_platforms:
                            social_platforms.append(platform)
                
                dynamic_media_weights["social_media_presence"]["platforms"] = social_platforms
                
                # æ¨¡æ“¬åª’é«”æåŠ
                recent_mentions = [
                    {
                        "source": "TechCrunch",
                        "date": (datetime.now() - timedelta(days=random.randint(1, 30))).strftime("%Y-%m-%d"),
                        "title": f"AI Innovation at {target_website}",
                        "sentiment": random.choice(["positive", "neutral", "positive"])
                    },
                    {
                        "source": "VentureBeat",
                        "date": (datetime.now() - timedelta(days=random.randint(1, 60))).strftime("%Y-%m-%d"),
                        "title": f"Digital Transformation Insights from {target_website}",
                        "sentiment": "positive"
                    }
                ]
                
                dynamic_media_weights["media_mentions"]["recent_mentions"] = recent_mentions
                
                # è¨ˆç®—åª’é«”è¦†è“‹åˆ†æ•¸
                positive_mentions = len([m for m in recent_mentions if m["sentiment"] == "positive"])
                total_mentions = len(recent_mentions)
                media_coverage_score = (positive_mentions / max(total_mentions, 1)) * 100
                
                dynamic_media_weights["media_mentions"]["media_coverage_score"] = media_coverage_score
                dynamic_media_weights["media_mentions"]["mention_sentiment"] = "positive" if media_coverage_score > 60 else "neutral"
                
                # æ¨¡æ“¬ç¤¾äº¤åª’é«”æ¬Šå¨åˆ†æ•¸
                social_authority_score = len(social_platforms) * 20 + random.randint(10, 30)
                dynamic_media_weights["social_media_presence"]["social_authority_score"] = min(social_authority_score, 100)
                
                # æ¨¡æ“¬åƒèˆ‡åº¦æŒ‡æ¨™
                dynamic_media_weights["social_media_presence"]["engagement_metrics"] = {
                    "total_followers": random.randint(1000, 50000),
                    "engagement_rate": random.uniform(2.0, 8.0),
                    "post_frequency": random.randint(3, 15)
                }
                
                if social_platforms:
                    st.success(f"âœ… ç™¼ç¾ç¤¾äº¤åª’é«”å¹³å°: {', '.join(social_platforms)}")
                else:
                    st.warning("âš ï¸ æœªç™¼ç¾ç¤¾äº¤åª’é«”é€£çµ")
                
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æå‹•æ…‹åª’é«”æ¬Šé‡: {str(e)}")
        
        return dynamic_media_weights
    
    def _analyze_competitor_benchmarks(self, target_website: str, competitors: List[str] = []) -> Dict:
        """åˆ†æç«¶çˆ­å°æ‰‹åŸºæº–"""
        st.write("ğŸ† åˆ†æç«¶çˆ­å°æ‰‹åŸºæº–...")
        
        if not competitors:
            # æ¨¡æ“¬ç«¶çˆ­å°æ‰‹
            competitors = [
                "competitor1.com",
                "competitor2.com",
                "competitor3.com"
            ]
        
        competitor_benchmarking = {
            "competitor_analysis": [],
            "market_position": "unknown",
            "competitive_advantages": [],
            "improvement_opportunities": []
        }
        
        try:
            # åˆ†æç›®æ¨™ç¶²ç«™
            target_analysis = self._analyze_single_competitor(target_website, "Target")
            
            # åˆ†æç«¶çˆ­å°æ‰‹
            competitor_analyses = []
            for competitor in competitors:
                try:
                    comp_analysis = self._analyze_single_competitor(competitor, f"Competitor {competitors.index(competitor) + 1}")
                    competitor_analyses.append(comp_analysis)
                except Exception as e:
                    st.warning(f"âš ï¸ ç„¡æ³•åˆ†æç«¶çˆ­å°æ‰‹ {competitor}: {str(e)}")
            
            competitor_benchmarking["competitor_analysis"] = [target_analysis] + competitor_analyses
            
            # è¨ˆç®—å¸‚å ´åœ°ä½
            target_score = target_analysis["overall_score"]
            competitor_scores = [comp["overall_score"] for comp in competitor_analyses if comp["overall_score"] > 0]
            
            if competitor_scores:
                avg_competitor_score = sum(competitor_scores) / len(competitor_scores)
                if target_score > avg_competitor_score * 1.2:
                    competitor_benchmarking["market_position"] = "leader"
                    st.success("ğŸ† å¸‚å ´é ˜å°è€…")
                elif target_score > avg_competitor_score:
                    competitor_benchmarking["market_position"] = "strong"
                    st.info("ğŸ’ª å¼·å‹¢ç«¶çˆ­è€…")
                elif target_score > avg_competitor_score * 0.8:
                    competitor_benchmarking["market_position"] = "average"
                    st.warning("ğŸ“Š å¹³å‡æ°´æº–")
                else:
                    competitor_benchmarking["market_position"] = "laggard"
                    st.error("âš ï¸ è½å¾Œç«¶çˆ­è€…")
            
            # è­˜åˆ¥ç«¶çˆ­å„ªå‹¢
            if target_analysis["ai_leader_score"] > 50:
                competitor_benchmarking["competitive_advantages"].append("AI Leadership")
            
            if target_analysis["social_authority_score"] > 60:
                competitor_benchmarking["competitive_advantages"].append("Strong Social Presence")
            
            if target_analysis["media_coverage_score"] > 70:
                competitor_benchmarking["competitive_advantages"].append("Positive Media Coverage")
            
            # è­˜åˆ¥æ”¹å–„æ©Ÿæœƒ
            if target_analysis["ai_leader_score"] < 30:
                competitor_benchmarking["improvement_opportunities"].append("Enhance AI Technology Integration")
            
            if target_analysis["social_authority_score"] < 40:
                competitor_benchmarking["improvement_opportunities"].append("Strengthen Social Media Presence")
            
            if target_analysis["media_coverage_score"] < 50:
                competitor_benchmarking["improvement_opportunities"].append("Improve Media Relations")
            
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•å®Œæˆç«¶çˆ­å°æ‰‹åŸºæº–åˆ†æ: {str(e)}")
        
        return competitor_benchmarking
    
    def _analyze_single_competitor(self, website: str, name: str) -> Dict:
        """åˆ†æå–®ä¸€ç«¶çˆ­å°æ‰‹"""
        if not website.startswith(('http://', 'https://')):
            website = 'https://' + website
        
        try:
            response = self.session.get(website, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ¨¡æ“¬åˆ†æçµæœ
                analysis = {
                    "name": name,
                    "website": website,
                    "ai_leader_score": random.randint(20, 90),
                    "social_authority_score": random.randint(30, 85),
                    "media_coverage_score": random.randint(40, 95),
                    "content_quality_score": random.randint(50, 95),
                    "overall_score": 0
                }
                
                # è¨ˆç®—ç¸½åˆ†
                analysis["overall_score"] = (
                    analysis["ai_leader_score"] * 0.3 +
                    analysis["social_authority_score"] * 0.25 +
                    analysis["media_coverage_score"] * 0.25 +
                    analysis["content_quality_score"] * 0.2
                )
                
                return analysis
            else:
                return {
                    "name": name,
                    "website": website,
                    "ai_leader_score": 0,
                    "social_authority_score": 0,
                    "media_coverage_score": 0,
                    "content_quality_score": 0,
                    "overall_score": 0
                }
        except Exception as e:
            return {
                "name": name,
                "website": website,
                "ai_leader_score": 0,
                "social_authority_score": 0,
                "media_coverage_score": 0,
                "content_quality_score": 0,
                "overall_score": 0,
                "error": str(e)
            }
    
    def _analyze_trends_and_predictions(self, target_website: str) -> Dict:
        """åˆ†æè¶¨å‹¢èˆ‡é æ¸¬"""
        st.write("ğŸ“ˆ åˆ†æè¶¨å‹¢èˆ‡é æ¸¬...")
        
        trend_analysis = {
            "current_trends": [],
            "predicted_growth": {},
            "market_opportunities": [],
            "risk_factors": []
        }
        
        try:
            # æ¨¡æ“¬ç•¶å‰è¶¨å‹¢
            current_trends = [
                "AI Integration Acceleration",
                "Voice Search Optimization",
                "Video Content Dominance",
                "Personalization at Scale",
                "Sustainability Focus"
            ]
            
            trend_analysis["current_trends"] = current_trends
            
            # æ¨¡æ“¬é æ¸¬æˆé•·
            trend_analysis["predicted_growth"] = {
                "ai_adoption_rate": random.uniform(15, 35),
                "content_consumption_growth": random.uniform(20, 40),
                "social_engagement_increase": random.uniform(10, 25),
                "market_share_growth": random.uniform(5, 15)
            }
            
            # è­˜åˆ¥å¸‚å ´æ©Ÿæœƒ
            market_opportunities = [
                "AI-Powered Content Personalization",
                "Voice-First Content Strategy",
                "Interactive Video Experiences",
                "Sustainability Messaging",
                "Micro-Moment Optimization"
            ]
            
            trend_analysis["market_opportunities"] = market_opportunities
            
            # è­˜åˆ¥é¢¨éšªå› ç´ 
            risk_factors = [
                "AI Regulation Changes",
                "Privacy Policy Updates",
                "Algorithm Changes",
                "Competitive Pressure",
                "Technology Disruption"
            ]
            
            trend_analysis["risk_factors"] = risk_factors
            
            st.success("âœ… è¶¨å‹¢åˆ†æå®Œæˆ")
            
        except Exception as e:
            st.warning(f"âš ï¸ ç„¡æ³•åˆ†æè¶¨å‹¢: {str(e)}")
        
        return trend_analysis
    
    def _generate_strategic_recommendations(self, ai_leader_analysis: Dict, dynamic_media_weights: Dict, 
                                         competitor_benchmarking: Dict, trend_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆç­–ç•¥å»ºè­°"""
        if not self.gemini_model:
            return self._generate_fallback_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
        
        st.write("ğŸ¯ ç”Ÿæˆç­–ç•¥å»ºè­°...")
        
        try:
            analysis_data = {
                "ai_leader_analysis": ai_leader_analysis,
                "dynamic_media_weights": dynamic_media_weights,
                "competitor_benchmarking": competitor_benchmarking,
                "trend_analysis": trend_analysis
            }
            
            prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ SIE ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­åˆ¶å®š E-E-A-T ç«¶çˆ­ç­–ç•¥ã€‚

è«‹æ ¹æ“šä»¥ä¸‹åˆ†æçµæœï¼Œæä¾›å…·é«”ã€å¯åŸ·è¡Œçš„ç­–ç•¥å»ºè­°ï¼š

åˆ†ææ•¸æ“šï¼š
{json.dumps(analysis_data, indent=2, ensure_ascii=False)}

è«‹ä»¥ JSON æ ¼å¼å›å‚³ç­–ç•¥å»ºè­°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "strategic_recommendations": [
    {{
      "strategy": "ç­–ç•¥åç¨±",
      "description": "ç­–ç•¥æè¿°",
      "priority": "High/Medium/Low",
      "timeline": "Short-term/Medium-term/Long-term",
      "expected_impact": "é æœŸå½±éŸ¿",
      "implementation_steps": ["æ­¥é©Ÿ1", "æ­¥é©Ÿ2", "æ­¥é©Ÿ3"]
    }}
  ]
}}

è«‹ç¢ºä¿å»ºè­°å…·é«”ã€å¯åŸ·è¡Œï¼Œä¸¦é‡å°ç«¶çˆ­å„ªå‹¢æå‡ã€‚
"""
            
            response = self.gemini_model.generate_content(prompt)
            recommendations_data = json.loads(response.text)
            return recommendations_data.get("strategic_recommendations", [])
            
        except Exception as e:
            st.warning(f"âš ï¸ Gemini API ç”Ÿæˆç­–ç•¥å»ºè­°å¤±æ•—: {str(e)}")
            return self._generate_fallback_strategic_recommendations(
                ai_leader_analysis, dynamic_media_weights, competitor_benchmarking, trend_analysis
            )
    
    def _generate_fallback_strategic_recommendations(self, ai_leader_analysis: Dict, dynamic_media_weights: Dict, 
                                                   competitor_benchmarking: Dict, trend_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆå‚™ç”¨ç­–ç•¥å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ AI é ˜å°è€…åˆ†æçš„å»ºè­°
        if ai_leader_analysis["ai_leader_score"] < 50:
            recommendations.append({
                "strategy": "AI Technology Integration",
                "description": "åŠ å¼· AI æŠ€è¡“æ•´åˆï¼Œæå‡ AI é ˜å°è€…åœ°ä½",
                "priority": "High",
                "timeline": "Medium-term",
                "expected_impact": "æå‡ AI é ˜å°è€…åˆ†æ•¸ 30-50%",
                "implementation_steps": [
                    "è©•ä¼°ç¾æœ‰ AI æŠ€è¡“åŸºç¤",
                    "åˆ¶å®š AI æ•´åˆè·¯ç·šåœ–",
                    "å¯¦æ–½ AI å…§å®¹ç”Ÿæˆå·¥å…·",
                    "å»ºç«‹ AI æ•ˆèƒ½ç›£æ§ç³»çµ±"
                ]
            })
        
        # åŸºæ–¼ç¤¾äº¤åª’é«”åˆ†æçš„å»ºè­°
        if dynamic_media_weights["social_media_presence"]["social_authority_score"] < 60:
            recommendations.append({
                "strategy": "Social Media Authority Building",
                "description": "å»ºç«‹å¼·å¤§çš„ç¤¾äº¤åª’é«”æ¬Šå¨ï¼Œæå‡å“ç‰Œå½±éŸ¿åŠ›",
                "priority": "Medium",
                "timeline": "Long-term",
                "expected_impact": "æå‡ç¤¾äº¤åª’é«”æ¬Šå¨åˆ†æ•¸ 40-60%",
                "implementation_steps": [
                    "åˆ¶å®šç¤¾äº¤åª’é«”å…§å®¹ç­–ç•¥",
                    "å»ºç«‹å½±éŸ¿è€…åˆä½œé—œä¿‚",
                    "å¯¦æ–½ç¤¾ç¾¤ç®¡ç†å·¥å…·",
                    "å®šæœŸç™¼å¸ƒé«˜åƒ¹å€¼å…§å®¹"
                ]
            })
        
        # åŸºæ–¼ç«¶çˆ­åˆ†æçš„å»ºè­°
        if competitor_benchmarking["market_position"] in ["average", "laggard"]:
            recommendations.append({
                "strategy": "Competitive Differentiation",
                "description": "å»ºç«‹ç¨ç‰¹çš„ç«¶çˆ­å„ªå‹¢ï¼Œè¶…è¶Šç«¶çˆ­å°æ‰‹",
                "priority": "High",
                "timeline": "Medium-term",
                "expected_impact": "æå‡å¸‚å ´åœ°ä½è‡³å¼·å‹¢ç«¶çˆ­è€…",
                "implementation_steps": [
                    "è­˜åˆ¥ç¨ç‰¹åƒ¹å€¼ä¸»å¼µ",
                    "é–‹ç™¼å·®ç•°åŒ–å…§å®¹ç­–ç•¥",
                    "å»ºç«‹å“ç‰Œæ¬Šå¨",
                    "å¯¦æ–½å‰µæ–°æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ"
                ]
            })
        
        return recommendations

def run_eeat_benchmarking(target_website: str, competitors: List[str] = [], gemini_api_key: Optional[str] = None) -> Dict:
    """åŸ·è¡Œ E-E-A-T åŸºæº–åˆ†æçš„ä¸»å‡½å¼"""
    analyzer = EEATBenchmarkingAnalyzer(gemini_api_key)
    return analyzer.analyze_eeat_benchmarking(target_website, competitors) 