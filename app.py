import streamlit as st
import json
import datetime
import random
import time
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from dateutil import parser as date_parser

try:
    import wikipediaapi
except ImportError:
    print("è­¦å‘Š: 'wikipedia-api' å‡½å¼åº«æœªå®‰è£ã€‚ç¶­åŸºç™¾ç§‘æª¢æŸ¥åŠŸèƒ½å°‡ç„¡æ³•é‹ä½œã€‚")
    print("è«‹åŸ·è¡Œ: pip install wikipedia-api")
    wikipediaapi = None

# é é¢è¨­å®š
st.set_page_config(
    page_title="E-E-A-T åˆ†æå·¥å…·",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªè¨‚ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #667eea;
    }
    .score-high { color: #28a745; }
    .score-medium { color: #ffc107; }
    .score-low { color: #dc3545; }
</style>
""", unsafe_allow_html=True)

def mock_google_custom_search(query: str, media_type: str) -> dict:
    """æ¨¡æ“¬ Google Custom Search JSON API çš„å›æ‡‰ã€‚"""
    time.sleep(0.1)
    
    results = {
        "industry_news": [
            {
                "title": f"{query} ç™¼å¸ƒé©å‘½æ€§æ ¸å¿ƒç”¢å“Bï¼Œå¼•é ˜ç”¢æ¥­æ–°æ¨™æº–",
                "link": f"https://news.example-industry.com/{query.lower()}-product-b-launch",
                "snippet": f"å°ç£é ˜å…ˆå“ç‰Œ {query} ä»Šæ—¥å®£å¸ƒæ¨å‡ºå…¶åŠƒæ™‚ä»£çš„æ ¸å¿ƒç”¢å“Bï¼Œè©²ç”¢å“æ¡ç”¨æœ€æ–°AIæ™¶ç‰‡ï¼Œæ•ˆèƒ½æå‡200%ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-07-01T10:00:00Z"}]}
            },
            {
                "title": f"å°ˆå®¶åˆ†æï¼š{query} çš„å¸‚å ´ç­–ç•¥å¦‚ä½•é¡›è¦†ç¾ç‹€",
                "link": f"https://analysis.example-industry.com/{query.lower()}-strategy",
                "snippet": f"ç”¢æ¥­åˆ†æå¸«æŒ‡å‡ºï¼Œ{query} è¿‘æœŸçš„å¤šè§’åŒ–ç¶“ç‡Ÿç­–ç•¥ï¼Œç‰¹åˆ¥æ˜¯åœ¨ç¶ è‰²èƒ½æºé ˜åŸŸçš„æŠ•å…¥ï¼Œå±•ç¾å…¶å¼·å¤§ä¼åœ–å¿ƒã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-15T14:30:00Z"}]}
            },
            {
                "title": f"{query} æ¦®ç²å¹´åº¦æœ€ä½³å‰µæ–°ä¼æ¥­ç",
                "link": f"https://awards.example-industry.com/{query.lower()}-innovation-2025",
                "snippet": f"åœ¨å¹´åº¦ç”¢æ¥­è©•é¸ä¸­ï¼Œ{query} æ†‘è—‰å…¶å‰µæ–°çš„æŠ€è¡“ç ”ç™¼å’Œå¸‚å ´è¡¨ç¾ï¼Œæ¦®ç²æœ€ä½³å‰µæ–°ä¼æ¥­çé …ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-20T09:00:00Z"}]}
            }
        ],
        "mainstream_news": [
            {
                "title": f"{query} é€£çºŒä¸‰å¹´æ¦®ç²æœ€ä½³é›‡ä¸»ç",
                "link": f"https://mainstream.example.com/{query.lower()}-best-employer-2025",
                "snippet": f"çŸ¥åäººåŠ›è³‡æºé¡§å•å…¬å¸å…¬å¸ƒå¹´åº¦æœ€ä½³é›‡ä¸»ï¼Œ{query} å› å…¶å„ªç•°çš„å“¡å·¥ç¦åˆ©èˆ‡ä¼æ¥­æ–‡åŒ–å†æ¬¡ä¸Šæ¦œã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-05-20T11:00:00Z"}]}
            },
            {
                "title": f"{query} è‚¡åƒ¹ä»Šæ—¥å°å¹…æ³¢å‹•",
                "link": f"https://finance.example.com/{query.lower()}-stock-today",
                "snippet": f"å—åˆ°åœ‹éš›å¸‚å ´å½±éŸ¿ï¼Œ{query} è‚¡åƒ¹ä»Šæ—¥æ”¶ç›¤æ™‚ä¸‹è·Œ 0.5%ï¼Œå¸‚å ´æ™®éèªç‚ºå±¬æ–¼æ­£å¸¸æŠ€è¡“æ€§å›æª”ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-07-02T08:00:00Z"}]}
            },
            {
                "title": f"æ¶ˆè²»è€…å ±å‘Šï¼š{query} å®¢æˆ¶æœå‹™æ»¿æ„åº¦èª¿æŸ¥",
                "link": f"https://consumer.example.com/{query.lower()}-service-review",
                "snippet": f"ä¸€ä»½æœ€æ–°çš„å ±å‘Šé¡¯ç¤ºï¼Œç´„æœ‰ 5% çš„ä½¿ç”¨è€…å›å ±æ ¸å¿ƒç”¢å“Båœ¨ç‰¹å®šæƒ…æ³ä¸‹æœ‰éç†±å•é¡Œï¼Œ{query}å®˜æ–¹å·²å›æ‡‰å°‡æä¾›è»Ÿé«”æ›´æ–°ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-10T18:00:00Z"}]}
            },
            {
                "title": f"{query} å®£å¸ƒæ“´å¤§æŠ•è³‡ç ”ç™¼ä¸­å¿ƒ",
                "link": f"https://business.example.com/{query.lower()}-rd-investment",
                "snippet": f"{query} ä»Šæ—¥å®£å¸ƒå°‡æŠ•è³‡ 50 å„„å…ƒæ“´å»ºç ”ç™¼ä¸­å¿ƒï¼Œé è¨ˆå°‡å‰µé€  500 å€‹å°±æ¥­æ©Ÿæœƒã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-25T14:00:00Z"}]}
            }
        ],
        "social_media": [
            {
                "title": f"Dcard ç¶²å‹ç†±è­° {query} çš„æ–°åŠŸèƒ½ï¼ŒCPå€¼è¶…é«˜ï¼",
                "link": f"https://dcard.tw/f/tech/p/123456789",
                "snippet": f"æœ€è¿‘å‰›å…¥æ‰‹{query}çš„æ ¸å¿ƒç”¢å“Bï¼ŒçœŸå¿ƒè¦ºå¾—ä¸éŒ¯ï¼Œæ“ä½œå¾ˆé †æš¢ï¼Œè€Œä¸”å¤–å‹ä¹Ÿå¥½çœ‹ï¼Œä¸çŸ¥é“å¤§å®¶è¦ºå¾—å¦‚ä½•ï¼Ÿ #é–‹ç®± #{query}",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-28T22:15:00Z"}]}
            },
            {
                "title": f"PTT MobileCommç‰ˆ - {query} ç”¢å“Bç½æƒ…å›å ±ï¼Ÿ",
                "link": f"https://www.ptt.cc/bbs/MobileComm/M.1234567890.A.ABC.html",
                "snippet": f"æˆ‘çš„{query}ç”¢å“Bç”¨äº†ä¸€é€±ï¼Œæ„Ÿè¦ºé›»æ± çºŒèˆªåŠ›æ²’æœ‰æƒ³åƒä¸­å¥½ï¼Œæœ‰äººä¹Ÿä¸€æ¨£å—ï¼Ÿé‚„æ˜¯æˆ‘æ‹¿åˆ°æ©Ÿç‹äº†...",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-25T13:00:00Z"}]}
            },
            {
                "title": f"Facebook ç¶²å‹åˆ†äº« {query} ä½¿ç”¨å¿ƒå¾—",
                "link": f"https://facebook.com/groups/tech/posts/123456789",
                "snippet": f"ç”¨äº†{query}çš„ç”¢å“ä¸‰å€‹æœˆï¼Œæ•´é«”ä¾†èªªå¾ˆæ»¿æ„ï¼Œå®¢æœä¹Ÿå¾ˆå°ˆæ¥­ï¼Œæ¨è–¦çµ¦å¤§å®¶ï¼",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-30T10:30:00Z"}]}
            }
        ],
        "video_sites": [
            {
                "title": f"ã€çŸ¥å YouTuberã€‘{query} æ ¸å¿ƒç”¢å“B æ·±åº¦é–‹ç®±ï¼çœŸçš„å€¼å¾—è²·å—ï¼Ÿ",
                "link": f"https://youtube.com/watch?v=abcdef123",
                "snippet": f"é€™æ¬¡æˆ‘å€‘æ¶å…ˆæ‹¿åˆ°äº† {query} çš„å¹´åº¦æ——è‰¦ç”¢å“Bï¼Œå¾å¤–è§€è¨­è¨ˆåˆ°å…§éƒ¨æ•ˆèƒ½ï¼Œé€²è¡Œä¸€å€‹å…¨é¢çš„å¯¦æ¸¬ï¼",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-20T20:00:00Z"}]}
            },
            {
                "title": f"ã€ç§‘æŠ€é »é“ã€‘{query} ç”¢å“è©•æ¸¬ï¼šæ€§åƒ¹æ¯”ä¹‹ç‹ï¼Ÿ",
                "link": f"https://youtube.com/watch?v=defghi456",
                "snippet": f"æ·±å…¥åˆ†æ {query} æœ€æ–°ç”¢å“çš„å„ªç¼ºé»ï¼Œçœ‹çœ‹æ˜¯å¦çœŸçš„å€¼å¾—å…¥æ‰‹ï¼",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-22T15:00:00Z"}]}
            }
        ],
        "ecommerce_retail": [
            {
                "title": f"PChome {query} ç”¢å“ç†±éŠ·ä¸­",
                "link": f"https://pchome.com.tw/prod/123456",
                "snippet": f"{query} ç”¢å“åœ¨ PChome 24h è³¼ç‰©ç†±éŠ·ï¼Œç¶²å‹è©•åƒ¹å¹³å‡ 4.5 é¡†æ˜Ÿã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-15T12:00:00Z"}]}
            }
        ]
    }
    
    if random.random() > 0.8 and media_type not in ["industry_news", "mainstream_news"]:
        return {"items": []}
        
    return {"items": results.get(media_type, [])}

def mock_gemini_api(snippet: str, official_info: str) -> str:
    """æ¨¡æ“¬ Gemini API é€²è¡Œå…§å®¹æ­£ç¢ºæ€§æ¯”å°ã€‚"""
    time.sleep(0.05)
    
    negative_keywords = ["éç†±", "ç½æƒ…", "ä¸‹è·Œ", "é›»æ± çºŒèˆªåŠ›æ²’æœ‰æƒ³åƒä¸­å¥½", "å•é¡Œ", "æ•…éšœ"]
    if any(keyword in snippet for keyword in negative_keywords):
        return "Uncertain"
    
    positive_keywords = ["é©å‘½æ€§", "æ–°åŠŸèƒ½", "æœ€ä½³é›‡ä¸»", "CPå€¼è¶…é«˜", "æ»¿æ„", "æ¨è–¦", "å„ªç§€"]
    if any(keyword in snippet for keyword in positive_keywords):
        return "Correct"
        
    return "Correct"

def check_wikipedia_presence(entities: list, user_agent: str) -> dict:
    """ä½¿ç”¨ Wikipedia API æª¢æŸ¥å“ç‰Œå’Œç›¸é—œå¯¦é«”æ˜¯å¦è¢«ç¶­åŸºç™¾ç§‘æ”¶éŒ„ã€‚"""
    if not wikipediaapi:
        return {"brand_found": False, "related_entities_found": []}

    wiki_client = wikipediaapi.Wikipedia(language='zh-tw', user_agent=user_agent)
    presence = {"brand_found": False, "related_entities_found": []}
    
    brand_name = entities[0]
    page_brand = wiki_client.page(brand_name)
    if page_brand.exists():
        presence["brand_found"] = True

    for entity in entities[1:]:
        page_entity = wiki_client.page(entity)
        if page_entity.exists():
            presence["related_entities_found"].append(entity)
            
    return presence

def analyze_media_mentions(brand_name: str, related_entities: list, media_weights: dict, official_info: str) -> dict:
    """éæ­·å„åª’é«”é¡å‹ï¼Œåˆ†æåª’é«”æåŠä¸¦é€²è¡Œæ­£ç¢ºæ€§æª¢æŸ¥ã€‚"""
    all_mentions = {}
    search_entities = [brand_name] + related_entities

    for media_type in media_weights.keys():
        all_mentions[media_type] = []
        for entity in search_entities:
            response = mock_google_custom_search(entity, media_type)
            
            if "items" in response:
                for item in response["items"]:
                    try:
                        date_str = item.get("pagemap", {}).get("metatags", [{}])[0].get("article:published_time", "")
                        parsed_date = date_parser.parse(date_str).date()
                    except (ValueError, TypeError):
                        parsed_date = datetime.date(1970, 1, 1)

                    accuracy = mock_gemini_api(item.get("snippet", ""), official_info)

                    all_mentions[media_type].append({
                        "title": item.get("title", "ç„¡æ¨™é¡Œ"),
                        "url": item.get("link", "#"),
                        "date_obj": parsed_date,
                        "date": parsed_date.isoformat() if parsed_date != datetime.date(1970, 1, 1) else "N/A",
                        "accuracy_check": accuracy,
                        "snippet": item.get("snippet", "")
                    })

    mentions_by_type = []
    total_mentions = 0
    for media_type, mentions in all_mentions.items():
        if not mentions:
            mentions_by_type.append({
                "type": media_type,
                "count": 0,
                "weight": media_weights[media_type],
                "latest_mention": None
            })
            continue

        mentions.sort(key=lambda x: x["date_obj"], reverse=True)
        
        count = len(mentions)
        total_mentions += count
        latest = mentions[0]
        
        mentions_by_type.append({
            "type": media_type,
            "count": count,
            "weight": media_weights[media_type],
            "latest_mention": {
                "title": latest["title"],
                "url": latest["url"],
                "date": latest["date"],
                "accuracy_check": latest["accuracy_check"]
            }
        })

    return {
        "total_mentions": total_mentions,
        "mentions_by_type": mentions_by_type,
        "raw_mentions": all_mentions
    }

def calculate_eeat_scores(media_analysis: dict, wiki_presence: dict, uses_https: bool) -> dict:
    """æ ¹æ“šåª’é«”åˆ†æã€ç¶­åŸºç™¾ç§‘æ”¶éŒ„å’Œ HTTPS ä½¿ç”¨æƒ…æ³è¨ˆç®— E-E-A-T åˆ†æ•¸ã€‚"""
    scores = {
        "experience": 0, "expertise": 0, "authoritativeness": 0, "trustworthiness": 0
    }
    
    # 1. Authoritativeness (æ¬Šå¨æ€§)
    auth_score = 0
    for item in media_analysis["mentions_by_type"]:
        auth_score += item["count"] * item["weight"]
    if wiki_presence["brand_found"]:
        auth_score += 20
    auth_score += len(wiki_presence["related_entities_found"]) * 10
    scores["authoritativeness"] = min(100, int(auth_score))

    # 2. Expertise (å°ˆæ¥­æ€§)
    industry_mentions = media_analysis["raw_mentions"].get("industry_news", [])
    scores["expertise"] = min(100, len(industry_mentions) * 5)

    # 3. Experience (ç¶“é©—)
    social_mentions = media_analysis["raw_mentions"].get("social_media", [])
    video_mentions = media_analysis["raw_mentions"].get("video_sites", [])
    experience_mentions = social_mentions + video_mentions
    scores["experience"] = min(100, len(experience_mentions) * 2)

    # 4. Trustworthiness (ä¿¡ä»»åº¦)
    trust_score = 0
    if uses_https:
        trust_score += 40
    mainstream_mentions = media_analysis["raw_mentions"].get("mainstream_news", [])
    if mainstream_mentions:
        positive_or_neutral_count = sum(1 for m in mainstream_mentions if m["accuracy_check"] == "Correct")
        negative_count = len(mainstream_mentions) - positive_or_neutral_count
        
        trust_score += positive_or_neutral_count * 5
        trust_score -= negative_count * 15
    
    scores["trustworthiness"] = max(0, min(100, int(trust_score)))
    
    # 5. Overall Score (ç¸½åˆ†)
    overall = sum(scores.values()) / len(scores)
    scores["overall_score"] = int(overall)
    
    return scores

def generate_recommendations(scores: dict, media_analysis: dict, wiki_presence: dict) -> list:
    """æ ¹æ“šåˆ†æçµæœç”Ÿæˆæ”¹é€²å»ºè­°ã€‚"""
    recommendations = []
    
    # Experience å»ºè­°
    if scores["experience"] < 50:
        recommendations.append({
            "category": "Experience",
            "priority": "é«˜",
            "title": "å¢åŠ ç¤¾ç¾¤åª’é«”æ›å…‰",
            "description": "å»ºè­°åŠ å¼·åœ¨ç¤¾ç¾¤åª’é«”å¹³å°çš„å“ç‰Œæ›å…‰ï¼ŒåŒ…æ‹¬ Facebookã€Instagramã€YouTube ç­‰å¹³å°ã€‚",
            "actions": ["å»ºç«‹å®˜æ–¹ç¤¾ç¾¤å¸³è™Ÿ", "å®šæœŸç™¼å¸ƒå…§å®¹", "èˆ‡ç¶²ç´…åˆä½œ", "èˆ‰è¾¦ç·šä¸Šæ´»å‹•"]
        })
    
    # Expertise å»ºè­°
    if scores["expertise"] < 60:
        recommendations.append({
            "category": "Expertise",
            "priority": "é«˜",
            "title": "æå‡ç”¢æ¥­å°ˆæ¥­å½¢è±¡",
            "description": "éœ€è¦å¢åŠ åœ¨ç”¢æ¥­åª’é«”çš„æ›å…‰åº¦ï¼Œå»ºç«‹å°ˆæ¥­æ¬Šå¨å½¢è±¡ã€‚",
            "actions": ["ç™¼å¸ƒç”¢æ¥­ç™½çš®æ›¸", "åƒèˆ‡ç”¢æ¥­è«–å£‡", "èˆ‡å°ˆæ¥­åª’é«”åˆä½œ", "å»ºç«‹æŠ€è¡“éƒ¨è½æ ¼"]
        })
    
    # Authoritativeness å»ºè­°
    if scores["authoritativeness"] < 70:
        recommendations.append({
            "category": "Authoritativeness",
            "priority": "ä¸­",
            "title": "å»ºç«‹ç¶­åŸºç™¾ç§‘é é¢",
            "description": "å»ºè­°ç‚ºå“ç‰Œå»ºç«‹ç¶­åŸºç™¾ç§‘é é¢ï¼Œæå‡æ¬Šå¨æ€§ã€‚",
            "actions": ["æº–å‚™å®Œæ•´çš„å“ç‰Œè³‡æ–™", "éµå¾ªç¶­åŸºç™¾ç§‘ç·¨è¼¯è¦ç¯„", "å®šæœŸæ›´æ–°å…§å®¹"]
        })
    
    # Trustworthiness å»ºè­°
    if scores["trustworthiness"] < 80:
        recommendations.append({
            "category": "Trustworthiness",
            "priority": "é«˜",
            "title": "æ”¹å–„å®¢æˆ¶æœå‹™",
            "description": "æ ¹æ“šè² é¢è©•è«–ï¼Œå»ºè­°æ”¹å–„å®¢æˆ¶æœå‹™å’Œç”¢å“å“è³ªã€‚",
            "actions": ["å»ºç«‹ 24/7 å®¢æœç³»çµ±", "æ”¹å–„ç”¢å“å“è³ª", "å¢åŠ å®¢æˆ¶å›é¥‹æ©Ÿåˆ¶", "å»ºç«‹å±æ©Ÿè™•ç†æµç¨‹"]
        })
    
    return recommendations

def create_radar_chart(scores: dict):
    """å»ºç«‹é›·é”åœ–ã€‚"""
    categories = ['Experience', 'Expertise', 'Authoritativeness', 'Trustworthiness']
    values = [scores['experience'], scores['expertise'], scores['authoritativeness'], scores['trustworthiness']]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill='toself',
        name='E-E-A-T åˆ†æ•¸',
        line_color='rgb(102, 126, 234)',
        fillcolor='rgba(102, 126, 234, 0.3)'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )),
        showlegend=False,
        title="E-E-A-T é›·é”åœ–åˆ†æ",
        height=400
    )
    
    return fig

def create_bar_chart(media_analysis: dict):
    """å»ºç«‹åª’é«”æåŠé•·æ¢åœ–ã€‚"""
    media_types = []
    counts = []
    
    for item in media_analysis["mentions_by_type"]:
        media_types.append(item["type"])
        counts.append(item["count"])
    
    df = pd.DataFrame({
        'åª’é«”é¡å‹': media_types,
        'æåŠæ¬¡æ•¸': counts
    })
    
    fig = px.bar(df, x='åª’é«”é¡å‹', y='æåŠæ¬¡æ•¸',
                 title="å„åª’é«”é¡å‹æåŠæ¬¡æ•¸",
                 color='æåŠæ¬¡æ•¸',
                 color_continuous_scale='Blues')
    
    fig.update_layout(height=400)
    return fig

def run_eeat_analysis(config_data: dict, module1_output: dict):
    """åŸ·è¡Œ E-E-A-T åˆ†æçš„ä¸»å‡½å¼ã€‚"""
    brand_name = config_data["brand_name"]
    related_entities = config_data["related_entities"]
    media_weights = config_data["media_weights"]
    official_info = config_data["official_info"]
    user_agent = "SIE-Diagnostic-Tool/1.0 (contact@example.com)"
    
    uses_https = module1_output.get("site_analysis", {}).get("uses_https", False)

    # æ­¥é©Ÿ 1: æª¢æŸ¥ç¶­åŸºç™¾ç§‘æ”¶éŒ„
    wiki_presence = check_wikipedia_presence([brand_name] + related_entities, user_agent)
    
    # æ­¥é©Ÿ 2: åˆ†æåª’é«”æåŠ
    media_analysis = analyze_media_mentions(brand_name, related_entities, media_weights, official_info)
    
    # æ­¥é©Ÿ 3: è¨ˆç®— E-E-A-T åˆ†æ•¸
    eeat_scores = calculate_eeat_scores(media_analysis, wiki_presence, uses_https)
    
    # æ­¥é©Ÿ 4: ç”Ÿæˆå»ºè­°
    recommendations = generate_recommendations(eeat_scores, media_analysis, wiki_presence)
    
    # æ­¥é©Ÿ 5: çµ„åˆæœ€çµ‚çµæœ
    result = {
        "eeat_scores": eeat_scores,
        "media_analysis": media_analysis,
        "wiki_presence": wiki_presence,
        "uses_https": uses_https,
        "recommendations": recommendations
    }
    
    return result

# ä¸»æ‡‰ç”¨ç¨‹å¼
def main():
    # æ¨™é¡Œå€åŸŸ
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ” E-E-A-T åˆ†æå·¥å…·</h1>
        <p>å°ˆæ¥­çš„å“ç‰Œæ¬Šå¨æ€§ã€å°ˆæ¥­æ€§ã€ç¶“é©—èˆ‡ä¿¡ä»»åº¦åˆ†æå¹³å°</p>
    </div>
    """, unsafe_allow_html=True)
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.markdown("---")
        
        # åŸºæœ¬è³‡è¨Š
        brand_name = st.text_input("å“ç‰Œåç¨±", "å°ç£å“ç‰ŒA")
        related_entities = st.text_area("ç›¸é—œå¯¦é«”ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰", "ç”¢å“B\né›†åœ˜C").splitlines()
        official_info = st.text_area("å®˜æ–¹è³‡è¨Š", "å°ç£å“ç‰ŒAæ˜¯å°ç£é ˜å…ˆçš„ç§‘æŠ€å…¬å¸ï¼Œä¸»åŠ›ç”¢å“ç‚ºç”¢å“Bã€‚")
        
        st.markdown("---")
        
        # åª’é«”æ¬Šé‡è¨­å®š
        st.subheader("ğŸ“Š åª’é«”æ¬Šé‡è¨­å®š")
        st.caption("æ•¸å­—è¶Šå¤§ä»£è¡¨è©²åª’é«”é¡å‹è¶Šé‡è¦")
        
        industry_news = st.slider("ç”¢æ¥­æ–°è", 0, 20, 10)
        mainstream_news = st.slider("ä¸»æµæ–°è", 0, 20, 8)
        social_media = st.slider("ç¤¾ç¾¤åª’é«”", 0, 20, 5)
        video_sites = st.slider("å½±éŸ³ç¶²ç«™", 0, 20, 5)
        ecommerce_retail = st.slider("é›»å•†é›¶å”®", 0, 20, 2)
        
        st.markdown("---")
        
        # æŠ€è¡“è¨­å®š
        st.subheader("ğŸ”§ æŠ€è¡“è¨­å®š")
        uses_https = st.checkbox("ç¶²ç«™æ”¯æ´ HTTPS", value=True)
        
        st.markdown("---")
        
        # åˆ†ææŒ‰éˆ•
        if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True):
            st.session_state.analyze = True
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    if 'analyze' in st.session_state and st.session_state.analyze:
        with st.spinner("ğŸ” æ­£åœ¨é€²è¡Œæ·±åº¦åˆ†æ..."):
            config_data = {
                "brand_name": brand_name,
                "related_entities": [e for e in related_entities if e.strip()],
                "media_weights": {
                    "industry_news": industry_news,
                    "mainstream_news": mainstream_news,
                    "social_media": social_media,
                    "video_sites": video_sites,
                    "ecommerce_retail": ecommerce_retail
                },
                "official_info": official_info
            }
            module1_output = {"site_analysis": {"uses_https": uses_https}}
            
            result = run_eeat_analysis(config_data, module1_output)
        
        # é¡¯ç¤ºçµæœ
        st.success("âœ… åˆ†æå®Œæˆï¼")
        
        # E-E-A-T åˆ†æ•¸å¡ç‰‡
        st.subheader("ğŸ¯ E-E-A-T è©•åˆ†çµæœ")
        scores = result["eeat_scores"]
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            score_class = "score-high" if scores['experience'] >= 70 else "score-medium" if scores['experience'] >= 40 else "score-low"
            st.markdown(f"""
            <div class="metric-card">
                <h3>ç¶“é©— (Experience)</h3>
                <h2 class="{score_class}">{scores['experience']}/100</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            score_class = "score-high" if scores['expertise'] >= 70 else "score-medium" if scores['expertise'] >= 40 else "score-low"
            st.markdown(f"""
            <div class="metric-card">
                <h3>å°ˆæ¥­ (Expertise)</h3>
                <h2 class="{score_class}">{scores['expertise']}/100</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            score_class = "score-high" if scores['authoritativeness'] >= 70 else "score-medium" if scores['authoritativeness'] >= 40 else "score-low"
            st.markdown(f"""
            <div class="metric-card">
                <h3>æ¬Šå¨ (Authoritativeness)</h3>
                <h2 class="{score_class}">{scores['authoritativeness']}/100</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            score_class = "score-high" if scores['trustworthiness'] >= 70 else "score-medium" if scores['trustworthiness'] >= 40 else "score-low"
            st.markdown(f"""
            <div class="metric-card">
                <h3>ä¿¡ä»» (Trustworthiness)</h3>
                <h2 class="{score_class}">{scores['trustworthiness']}/100</h2>
            </div>
            """, unsafe_allow_html=True)
        
        with col5:
            score_class = "score-high" if scores['overall_score'] >= 70 else "score-medium" if scores['overall_score'] >= 40 else "score-low"
            st.markdown(f"""
            <div class="metric-card">
                <h3>ç¸½åˆ† (Overall)</h3>
                <h2 class="{score_class}">{scores['overall_score']}/100</h2>
            </div>
            """, unsafe_allow_html=True)
        
        # è¦–è¦ºåŒ–åœ–è¡¨
        st.subheader("ğŸ“Š è¦–è¦ºåŒ–åˆ†æ")
        col1, col2 = st.columns(2)
        
        with col1:
            radar_fig = create_radar_chart(scores)
            st.plotly_chart(radar_fig, use_container_width=True)
        
        with col2:
            bar_fig = create_bar_chart(result["media_analysis"])
            st.plotly_chart(bar_fig, use_container_width=True)
        
        # è©³ç´°åˆ†æå ±å‘Š
        st.subheader("ğŸ“‹ è©³ç´°åˆ†æå ±å‘Š")
        
        # ç¶­åŸºç™¾ç§‘æª¢æŸ¥çµæœ
        with st.expander("ğŸŒ ç¶­åŸºç™¾ç§‘æ”¶éŒ„ç‹€æ³", expanded=True):
            wiki_presence = result["wiki_presence"]
            if wiki_presence["brand_found"]:
                st.success(f"âœ… å“ç‰Œ '{brand_name}' åœ¨ç¶­åŸºç™¾ç§‘æœ‰æ”¶éŒ„")
            else:
                st.warning(f"âš ï¸ å“ç‰Œ '{brand_name}' åœ¨ç¶­åŸºç™¾ç§‘ç„¡æ”¶éŒ„")
            
            if wiki_presence["related_entities_found"]:
                st.info(f"ğŸ“š ç›¸é—œå¯¦é«”æ”¶éŒ„ï¼š{', '.join(wiki_presence['related_entities_found'])}")
            else:
                st.info("ğŸ“š ç›¸é—œå¯¦é«”å‡ç„¡ç¶­åŸºç™¾ç§‘æ”¶éŒ„")
        
        # åª’é«”æåŠåˆ†æ
        with st.expander("ğŸ“° åª’é«”æåŠåˆ†æ", expanded=True):
            media_analysis = result["media_analysis"]
            st.write(f"**ç¸½æåŠæ¬¡æ•¸ï¼š{media_analysis['total_mentions']}**")
            
            for item in media_analysis["mentions_by_type"]:
                if item["count"] > 0:
                    st.write(f"- **{item['type']}**: {item['count']} æ¬¡æåŠ")
                    if item["latest_mention"]:
                        st.caption(f"  æœ€æ–°ï¼š{item['latest_mention']['title']}")
        
        # æ”¹é€²å»ºè­°
        if result["recommendations"]:
            st.subheader("ğŸ’¡ æ”¹é€²å»ºè­°")
            for i, rec in enumerate(result["recommendations"], 1):
                with st.expander(f"{i}. {rec['title']} ({rec['priority']}å„ªå…ˆç´š)", expanded=True):
                    st.write(f"**é¡åˆ¥ï¼š{rec['category']}**")
                    st.write(rec['description'])
                    st.write("**å»ºè­°è¡Œå‹•ï¼š**")
                    for action in rec['actions']:
                        st.write(f"- {action}")
        
        # åŸå§‹è³‡æ–™
        with st.expander("ğŸ” åŸå§‹åˆ†æè³‡æ–™", expanded=False):
            st.json(result)
        
        st.markdown("---")
        st.caption("æœ¬å·¥å…·ç”± Streamlit è£½ä½œï¼Œç¨‹å¼ç¢¼å·²é–‹æºæ–¼ GitHubã€‚")

if __name__ == "__main__":
    main() 