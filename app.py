import streamlit as st
import json
import datetime
import random
import time
from dateutil import parser as date_parser

try:
    import wikipediaapi
except ImportError:
    print("è­¦å‘Š: 'wikipedia-api' å‡½å¼åº«æœªå®‰è£ã€‚ç¶­åŸºç™¾ç§‘æª¢æŸ¥åŠŸèƒ½å°‡ç„¡æ³•é‹ä½œã€‚")
    print("è«‹åŸ·è¡Œ: pip install wikipedia-api")
    wikipediaapi = None

def mock_google_custom_search(query: str, media_type: str) -> dict:
    """æ¨¡æ“¬ Google Custom Search JSON API çš„å›æ‡‰ã€‚"""
    time.sleep(0.1) # æ¨¡æ“¬ç¶²è·¯å»¶é²
    
    results = {
        "industry_news": [
            {
                "title": f"{query} ç™¼å¸ƒé©å‘½æ€§æ ¸å¿ƒç”¢å“Bï¼Œå¼•é ˜ç”¢æ¥­æ–°æ¨™æº–",
                "link": f"https://news.example-industry.com/{query.lower()}-product-b-launch",
                "snippet": f"å°ç£é ˜å…ˆå“ç‰Œ {query} ä»Šæ—¥å®£å¸ƒæ¨å‡ºå…¶åŠƒæ™‚ä»£çš„æ ¸å¿ƒç”¢å“Bï¼Œè©²ç”¢å“æ¡ç”¨æœ€æ–°AIæ™¶ç‰‡ï¼Œæ•ˆèƒ½æå‡200%ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-07-01T10:00:00Z"}]}
            },
            {
                "title": f"å°ˆå®¶åˆ†æï¼š{query} çš„å¸‚å ´ç­–ç•¥å¦‚ä½•é¡›è¦†ç¾ç‹€",
                "link": f"https://analysis.example-industry.com/{query.lower()}-strategy",
                "snippet": f"ç”¢æ¥­åˆ†æå¸«æŒ‡å‡ºï¼Œ{query} è¿‘æœŸçš„å¤šè§’åŒ–ç¶“ç‡Ÿç­–ç•¥ï¼Œç‰¹åˆ¥æ˜¯åœ¨ç¶ è‰²èƒ½æºé ˜åŸŸçš„æŠ•å…¥ï¼Œå±•ç¾å…¶å¼·å¤§ä¼åœ–å¿ƒã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-15T14:30:00Z"}]}
            }
        ],
        "mainstream_news": [
            {
                "title": f"{query} é€£çºŒä¸‰å¹´æ¦®ç²æœ€ä½³é›‡ä¸»ç",
                "link": f"https://mainstream.example.com/{query.lower()}-best-employer-2025",
                "snippet": f"çŸ¥åäººåŠ›è³‡æºé¡§å•å…¬å¸å…¬å¸ƒå¹´åº¦æœ€ä½³é›‡ä¸»ï¼Œ{query} å› å…¶å„ªç•°çš„å“¡å·¥ç¦åˆ©èˆ‡ä¼æ¥­æ–‡åŒ–å†æ¬¡ä¸Šæ¦œã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-05-20T11:00:00Z"}]}
            },
            {
                "title": f"{query} è‚¡åƒ¹ä»Šæ—¥å°å¹…æ³¢å‹•",
                "link": f"https://finance.example.com/{query.lower()}-stock-today",
                "snippet": f"å—åˆ°åœ‹éš›å¸‚å ´å½±éŸ¿ï¼Œ{query} è‚¡åƒ¹ä»Šæ—¥æ”¶ç›¤æ™‚ä¸‹è·Œ 0.5%ï¼Œå¸‚å ´æ™®éèªç‚ºå±¬æ–¼æ­£å¸¸æŠ€è¡“æ€§å›æª”ã€‚",
                "pagemap": {"metatags": [{"article:published_time": "2025-07-02T08:00:00Z"}]}
            }
        ],
        "social_media": [
            {
                "title": f"Dcard ç¶²å‹ç†±è­° {query} çš„æ–°åŠŸèƒ½ï¼ŒCPå€¼è¶…é«˜ï¼",
                "link": f"https://dcard.tw/f/tech/p/123456789",
                "snippet": f"æœ€è¿‘å‰›å…¥æ‰‹{query}çš„æ ¸å¿ƒç”¢å“Bï¼ŒçœŸå¿ƒè¦ºå¾—ä¸éŒ¯ï¼Œæ“ä½œå¾ˆé †æš¢ï¼Œè€Œä¸”å¤–å‹ä¹Ÿå¥½çœ‹ï¼Œä¸çŸ¥é“å¤§å®¶è¦ºå¾—å¦‚ä½•ï¼Ÿ #é–‹ç®± #{query}",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-28T22:15:00Z"}]}
            }
        ],
        "video_sites": [
            {
                "title": f"ã€çŸ¥å YouTuberã€‘{query} æ ¸å¿ƒç”¢å“B æ·±åº¦é–‹ç®±ï¼çœŸçš„å€¼å¾—è²·å—ï¼Ÿ",
                "link": f"https://youtube.com/watch?v=abcdef123",
                "snippet": f"é€™æ¬¡æˆ‘å€‘æ¶å…ˆæ‹¿åˆ°äº† {query} çš„å¹´åº¦æ——è‰¦ç”¢å“Bï¼Œå¾å¤–è§€è¨­è¨ˆåˆ°å…§éƒ¨æ•ˆèƒ½ï¼Œé€²è¡Œä¸€å€‹å…¨é¢çš„å¯¦æ¸¬ï¼",
                "pagemap": {"metatags": [{"article:published_time": "2025-06-20T20:00:00Z"}]}
            }
        ],
        "ecommerce_retail": []
    }
    
    if random.random() > 0.8 and media_type not in ["industry_news", "mainstream_news"]:
        return {"items": []}
        
    return {"items": results.get(media_type, [])}

def mock_gemini_api(snippet: str, official_info: str) -> str:
    """æ¨¡æ“¬ Gemini API é€²è¡Œå…§å®¹æ­£ç¢ºæ€§æ¯”å°ã€‚"""
    time.sleep(0.05)
    
    negative_keywords = ["éç†±", "ç½æƒ…", "ä¸‹è·Œ", "é›»æ± çºŒèˆªåŠ›æ²’æœ‰æƒ³åƒä¸­å¥½"]
    if any(keyword in snippet for keyword in negative_keywords):
        return "Uncertain"
    
    positive_keywords = ["é©å‘½æ€§", "æ–°åŠŸèƒ½", "æœ€ä½³é›‡ä¸»", "CPå€¼è¶…é«˜"]
    if any(keyword in snippet for keyword in positive_keywords):
        return "Correct"
        
    return "Correct"

def check_wikipedia_presence(entities: list, user_agent: str) -> dict:
    """ä½¿ç”¨ Wikipedia API æª¢æŸ¥å“ç‰Œå’Œç›¸é—œå¯¦é«”æ˜¯å¦è¢«ç¶­åŸºç™¾ç§‘æ”¶éŒ„ã€‚"""
    if not wikipediaapi:
        return {"brand_found": False, "related_entities_found": []}

    wiki_client = wikipediaapi.Wikipedia(language='zh-tw', user_agent=user_agent)
    presence = {"brand_found": False, "related_entities_found": []}
    
    brand_name = entities[0]
    page_brand = wiki_client.page(brand_name)
    if page_brand.exists():
        presence["brand_found"] = True

    for entity in entities[1:]:
        page_entity = wiki_client.page(entity)
        if page_entity.exists():
            presence["related_entities_found"].append(entity)
            
    return presence

def analyze_media_mentions(brand_name: str, related_entities: list, media_weights: dict, official_info: str) -> dict:
    """éæ­·å„åª’é«”é¡å‹ï¼Œåˆ†æåª’é«”æåŠä¸¦é€²è¡Œæ­£ç¢ºæ€§æª¢æŸ¥ã€‚"""
    all_mentions = {}
    search_entities = [brand_name] + related_entities

    for media_type in media_weights.keys():
        all_mentions[media_type] = []
        for entity in search_entities:
            response = mock_google_custom_search(entity, media_type)
            
            if "items" in response:
                for item in response["items"]:
                    try:
                        date_str = item.get("pagemap", {}).get("metatags", [{}])[0].get("article:published_time", "")
                        parsed_date = date_parser.parse(date_str).date()
                    except (ValueError, TypeError):
                        parsed_date = datetime.date(1970, 1, 1)

                    accuracy = mock_gemini_api(item.get("snippet", ""), official_info)

                    all_mentions[media_type].append({
                        "title": item.get("title", "ç„¡æ¨™é¡Œ"),
                        "url": item.get("link", "#"),
                        "date_obj": parsed_date,
                        "date": parsed_date.isoformat() if parsed_date != datetime.date(1970, 1, 1) else "N/A",
                        "accuracy_check": accuracy,
                        "snippet": item.get("snippet", "")
                    })

    mentions_by_type = []
    total_mentions = 0
    for media_type, mentions in all_mentions.items():
        if not mentions:
            mentions_by_type.append({
                "type": media_type,
                "count": 0,
                "weight": media_weights[media_type],
                "latest_mention": None
            })
            continue

        mentions.sort(key=lambda x: x["date_obj"], reverse=True)
        
        count = len(mentions)
        total_mentions += count
        latest = mentions[0]
        
        mentions_by_type.append({
            "type": media_type,
            "count": count,
            "weight": media_weights[media_type],
            "latest_mention": {
                "title": latest["title"],
                "url": latest["url"],
                "date": latest["date"],
                "accuracy_check": latest["accuracy_check"]
            }
        })

    return {
        "total_mentions": total_mentions,
        "mentions_by_type": mentions_by_type,
        "raw_mentions": all_mentions
    }

def calculate_eeat_scores(media_analysis: dict, wiki_presence: dict, uses_https: bool) -> dict:
    """æ ¹æ“šåª’é«”åˆ†æã€ç¶­åŸºç™¾ç§‘æ”¶éŒ„å’Œ HTTPS ä½¿ç”¨æƒ…æ³è¨ˆç®— E-E-A-T åˆ†æ•¸ã€‚"""
    scores = {
        "experience": 0, "expertise": 0, "authoritativeness": 0, "trustworthiness": 0
    }
    
    # 1. Authoritativeness (æ¬Šå¨æ€§)
    auth_score = 0
    for item in media_analysis["mentions_by_type"]:
        auth_score += item["count"] * item["weight"]
    if wiki_presence["brand_found"]:
        auth_score += 20
    auth_score += len(wiki_presence["related_entities_found"]) * 10
    scores["authoritativeness"] = min(100, int(auth_score))

    # 2. Expertise (å°ˆæ¥­æ€§)
    industry_mentions = media_analysis["raw_mentions"].get("industry_news", [])
    scores["expertise"] = min(100, len(industry_mentions) * 5)

    # 3. Experience (ç¶“é©—)
    social_mentions = media_analysis["raw_mentions"].get("social_media", [])
    video_mentions = media_analysis["raw_mentions"].get("video_sites", [])
    experience_mentions = social_mentions + video_mentions
    scores["experience"] = min(100, len(experience_mentions) * 2)

    # 4. Trustworthiness (ä¿¡ä»»åº¦)
    trust_score = 0
    if uses_https:
        trust_score += 40
    mainstream_mentions = media_analysis["raw_mentions"].get("mainstream_news", [])
    if mainstream_mentions:
        positive_or_neutral_count = sum(1 for m in mainstream_mentions if m["accuracy_check"] == "Correct")
        negative_count = len(mainstream_mentions) - positive_or_neutral_count
        
        trust_score += positive_or_neutral_count * 5
        trust_score -= negative_count * 15
    
    scores["trustworthiness"] = max(0, min(100, int(trust_score)))
    
    # 5. Overall Score (ç¸½åˆ†)
    overall = sum(scores.values()) / len(scores)
    scores["overall_score"] = int(overall)
    
    return scores

def run_eeat_analysis(config_data: dict, module1_output: dict):
    """åŸ·è¡Œ E-E-A-T åˆ†æçš„ä¸»å‡½å¼ã€‚"""
    brand_name = config_data["brand_name"]
    related_entities = config_data["related_entities"]
    media_weights = config_data["media_weights"]
    official_info = config_data["official_info"]
    user_agent = "SIE-Diagnostic-Tool/1.0 (contact@example.com)"
    
    uses_https = module1_output.get("site_analysis", {}).get("uses_https", False)

    # æ­¥é©Ÿ 1: æª¢æŸ¥ç¶­åŸºç™¾ç§‘æ”¶éŒ„
    wiki_presence = check_wikipedia_presence([brand_name] + related_entities, user_agent)
    
    # æ­¥é©Ÿ 2: åˆ†æåª’é«”æåŠ
    media_analysis = analyze_media_mentions(brand_name, related_entities, media_weights, official_info)
    
    # æ­¥é©Ÿ 3: è¨ˆç®— E-E-A-T åˆ†æ•¸
    eeat_scores = calculate_eeat_scores(media_analysis, wiki_presence, uses_https)
    
    # æ­¥é©Ÿ 4: çµ„åˆæœ€çµ‚çš„ JSON è¼¸å‡º
    result = {
        "eeat_scores": eeat_scores,
        "media_analysis": media_analysis,
        "wiki_presence": wiki_presence,
        "uses_https": uses_https
    }
    
    return result

# Streamlit æ‡‰ç”¨ç¨‹å¼
st.set_page_config(page_title="E-E-A-T åˆ†æäº’å‹•ç¶²é ", layout="centered")
st.title("ğŸ” E-E-A-T åˆ†æäº’å‹•ç¶²é ç‰ˆ")
st.markdown("""
æœ¬å·¥å…·å¯å³æ™‚åˆ†æå“ç‰Œç¶²ç«™çš„ E-E-A-Tï¼ˆç¶“é©—ã€å°ˆæ¥­ã€æ¬Šå¨ã€ä¿¡ä»»ï¼‰åˆ†æ•¸ã€‚\
è«‹è¼¸å…¥ä¸‹åˆ—è³‡è¨Šä¸¦é»æ“Šã€Œåˆ†æã€ï¼
""")

with st.form("eeat_form"):
    brand_name = st.text_input("å“ç‰Œåç¨±", "å°ç£å“ç‰ŒA")
    related_entities = st.text_area("ç›¸é—œå¯¦é«”ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰", "ç”¢å“B\né›†åœ˜C").splitlines()
    official_info = st.text_area("å®˜æ–¹è³‡è¨Š", "å°ç£å“ç‰ŒAæ˜¯å°ç£é ˜å…ˆçš„ç§‘æŠ€å…¬å¸ï¼Œä¸»åŠ›ç”¢å“ç‚ºç”¢å“Bã€‚")
    st.markdown("**åª’é«”æ¬Šé‡è¨­å®š**ï¼ˆæ•¸å­—è¶Šå¤§ä»£è¡¨è¶Šé‡è¦ï¼‰")
    col1, col2, col3 = st.columns(3)
    with col1:
        industry_news = st.number_input("ç”¢æ¥­æ–°è", min_value=0, max_value=20, value=10)
        mainstream_news = st.number_input("ä¸»æµæ–°è", min_value=0, max_value=20, value=8)
    with col2:
        social_media = st.number_input("ç¤¾ç¾¤åª’é«”", min_value=0, max_value=20, value=5)
        video_sites = st.number_input("å½±éŸ³ç¶²ç«™", min_value=0, max_value=20, value=5)
    with col3:
        ecommerce_retail = st.number_input("é›»å•†é›¶å”®", min_value=0, max_value=20, value=2)
    uses_https = st.checkbox("ç¶²ç«™æ˜¯å¦æ”¯æ´ HTTPSï¼Ÿ", value=True)
    submitted = st.form_submit_button("åˆ†æ")

if submitted:
    config_data = {
        "brand_name": brand_name,
        "related_entities": [e for e in related_entities if e.strip()],
        "media_weights": {
            "industry_news": industry_news,
            "mainstream_news": mainstream_news,
            "social_media": social_media,
            "video_sites": video_sites,
            "ecommerce_retail": ecommerce_retail
        },
        "official_info": official_info
    }
    module1_output = {"site_analysis": {"uses_https": uses_https}}
    
    with st.spinner("åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
        result = run_eeat_analysis(config_data, module1_output)
    
    st.success("åˆ†æå®Œæˆï¼")
    
    # é¡¯ç¤º E-E-A-T åˆ†æ•¸
    st.subheader("ğŸ¯ E-E-A-T åˆ†æ•¸")
    scores = result["eeat_scores"]
    
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("ç¶“é©— (Experience)", f"{scores['experience']}/100")
    with col2:
        st.metric("å°ˆæ¥­ (Expertise)", f"{scores['expertise']}/100")
    with col3:
        st.metric("æ¬Šå¨ (Authoritativeness)", f"{scores['authoritativeness']}/100")
    with col4:
        st.metric("ä¿¡ä»» (Trustworthiness)", f"{scores['trustworthiness']}/100")
    with col5:
        st.metric("ç¸½åˆ† (Overall)", f"{scores['overall_score']}/100")
    
    # é¡¯ç¤ºè©³ç´°åˆ†æçµæœ
    with st.expander("ğŸ“Š è©³ç´°åˆ†æçµæœï¼ˆé»æ“Šå±•é–‹ï¼‰"):
        st.json(result)
    
    st.markdown("---")
    st.caption("æœ¬å·¥å…·ç”± Streamlit è£½ä½œï¼Œç¨‹å¼ç¢¼å·²é–‹æºæ–¼ GitHubã€‚") 